{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Read Input Files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa', 'train_features.csv'))\ntest_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa' , 'test_features.csv'))\n\ntrain_target = pd.read_csv(os.path.join('/kaggle/input/lish-moa','train_targets_scored.csv'))","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Print Shape of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_data.shap: {}  , train_target.shap: {}  , test_data.shap: {} \".format(train_data.shape , train_target.shape , test_data.shape))","execution_count":164,"outputs":[{"output_type":"stream","text":"train_data.shap: (23814, 876)  , train_target.shap: (23814, 207)  , test_data.shap: (3982, 876) \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGTypeFeature = sum(train_data.columns.to_series().str.contains('g-') == True )\nNCTypeFeature = sum(train_data.columns.to_series().str.contains('c-') == True )\n\nprint(\"NGTypeFeature = {} \\nNCTypeFeature = {}\".format(NGTypeFeature , NCTypeFeature))","execution_count":165,"outputs":[{"output_type":"stream","text":"NGTypeFeature = 772 \nNCTypeFeature = 100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Define a Neural Network To Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\nclass MultiLabelClassifer(nn.Module):\n    #TO-DO: Documentation\n    def __init__(self , input_feature , hidden_dim , output_dim):\n        super(MultiLabelClassifer , self).__init__()\n        \n        self.fc1 = nn.Linear(in_features = input_feature , out_features = 2000)\n        #TO-DO: As close features are related here, convoluation layer will help here\n        #TO-DO: Later add and play with Convolutatin layer, to see accurracy difference\n        self.fc2 = nn.Linear( in_features = 2000 , out_features  = 4000)\n        self.fc3 = nn.Linear( in_features = 4000 , out_features  = 1000)\n        self.fc4 = nn.Linear(in_features = 1000 , out_features = 400)\n        self.fc5 = nn.Linear(in_features = 400 , out_features = output_dim)\n        self.drop = nn.Dropout(0.4) # dropout with 30% prob\n        \n        self.sig = nn.Sigmoid()\n        #TO-DO: Define output layer so that it be as per Question requirement\n    def forward(self, x):\n        \n        # (1) input layer\n        \n        t = x\n        \n        \n        # (2) Hidden Linear Layer\n        \n        t = self.fc1(t)\n        t = F.relu(t)\n        #3\n        \n        t = self.fc2(t)\n        t = F.relu(t)\n        #4\n        \n        t= self.fc3(t)\n        t = F.relu(t)\n        t = self.drop(t)\n        \n        #5\n        \n        t = self.fc4(t)\n        t = F.relu(t)\n        #6\n        \n        t = self.drop(t)\n        \n        t = self.fc5(t)\n        t = F.relu(t)\n        \n        # (7) Output Layer\n        \n        t = self.sig(t)\n        \n        return t\n        ","execution_count":166,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rf = train_data.copy()\ntrain_data.head()\n\ntest_rf = test_data.copy()","execution_count":167,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One hot coding of input feature and Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data\nimport torch\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,Normalizer\n\nignore_columns = ['sig_id','cp_type']\n\ntrain_columns = [x for x in train_data.columns if x not in ignore_columns]\ntrain = train_rf[train_columns]\n#print(train.head())\ntest = test_rf[train_columns]\ntarget = train_target.iloc[:,1:].values\n\ntransform = ColumnTransformer([('o',OneHotEncoder(),[0,1]),('s',Normalizer(),list(range(3,train.shape[1])))])\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)\n","execution_count":168,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_rf['cp_dose'] = train_data['cp_dose'].replace({\"D1\":1 , \"D2\":2})\n#train_rf['cp_type'] = train_rf['cp_type'].replace({\"trt_cp\":1 , \"ctl_vehicle\":2})\n\n#test_rf['cp_dose'] = test_data['cp_dose'].replace({\"D1\":1 , \"D2\":2})\n#test_rf['cp_type'] = test_data['cp_type'].replace({\"trt_cp\":1 , \"ctl_vehicle\":2})","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[0])","execution_count":177,"outputs":[{"output_type":"stream","text":"[ 1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n  0.00000000e+00  2.50303421e-02 -1.11260925e-02 -2.78623567e-02\n -8.72493903e-03 -4.54199501e-02 -4.58687638e-02 -1.46313278e-03\n  2.49001861e-02 -4.13357451e-03  5.30946650e-02  6.86685016e-03\n  2.50168776e-02 -1.80198715e-02  8.02927774e-03 -2.92985607e-02\n -3.57659666e-02  2.84637671e-02  7.97990823e-03 -1.65791794e-02\n -2.55285253e-02 -5.09852404e-02 -5.33190718e-02  3.11476733e-02\n  1.97163874e-02  1.19563979e-02  8.55887795e-03  7.30668762e-03\n -1.28046559e-02  2.61164713e-02  1.31681950e-02 -2.50617590e-02\n -4.11113382e-03 -1.35092935e-02 -6.89826712e-03  9.86492592e-03\n  1.33073273e-02 -2.26875344e-02 -2.29747751e-02 -9.70335297e-03\n -1.55738366e-03 -1.15165605e-02 -5.37678856e-02  1.39850360e-02\n -4.63624589e-02  9.90980729e-03 -5.12994100e-03 -1.12652248e-03\n  1.57084808e-03  1.89982854e-02 -5.48001572e-03 -2.21130528e-02\n  2.74629125e-02  2.11615677e-02 -1.57533621e-03 -7.20346046e-03\n -3.02949272e-02  1.01566548e-02  2.59863153e-02  4.30322610e-02\n  2.72026005e-02 -5.83457857e-03  2.96710761e-02 -1.40747988e-02\n -9.06603747e-03  1.12652248e-03  7.49518939e-03 -1.88277362e-02\n  2.94017878e-02 -3.31897758e-02 -3.16279040e-02 -2.40968095e-02\n  1.40927513e-03 -2.35133516e-02 -1.52865958e-02 -2.84188858e-02\n -4.63624589e-02 -1.68888609e-02 -3.83735744e-02  3.29788333e-02\n -9.99059376e-03 -8.79226109e-03 -2.24406868e-05 -9.62256650e-02\n  7.77345391e-02  8.52746098e-04 -5.33190718e-03  1.81769563e-02\n -7.10920958e-02 -1.35137816e-02  9.19170531e-03 -5.04017826e-02\n -6.99700614e-02 -8.06967097e-02  3.14932599e-02 -4.66317472e-02\n  4.29963559e-03  2.03177978e-02  2.44827893e-02 -3.71617773e-03\n -2.40743688e-02  2.47834945e-02 -3.48638510e-02 -3.61025769e-02\n -5.30049022e-03 -5.63710052e-02  7.14960281e-03 -3.14079852e-02\n -7.84077597e-02  3.19555380e-02 -3.42669287e-02 -1.30155983e-03\n  5.68647003e-02  2.12468423e-02 -4.44415361e-02  1.97074111e-02\n -5.33190718e-03 -1.56187180e-03  2.09865303e-02  8.86407129e-02\n  2.44603486e-03  5.78072092e-02  5.50245640e-02  2.18123476e-03\n -3.86967203e-02 -5.73135141e-02 -8.12352862e-03 -1.16691571e-02\n -2.30241447e-02 -1.01880718e-03 -1.01297260e-02 -3.29833215e-02\n  1.39850360e-02 -3.93789172e-02  1.58835181e-02  1.27148931e-02\n -1.57129689e-02 -1.96221365e-02 -2.08518862e-02 -2.21713986e-02\n  2.37736636e-02  5.57426660e-02 -5.32741905e-02  1.96670179e-02\n  5.09852404e-02 -7.89014548e-03  1.73152339e-02 -5.62363611e-02\n -7.98439636e-03  2.53804168e-02 -7.61188096e-03  6.57063309e-03\n -8.85509501e-03 -2.22566732e-02  1.19160047e-02  1.92047398e-02\n -9.06603747e-04 -4.89655786e-03 -3.50029833e-02 -3.61833634e-02\n -1.45370769e-02  1.08208992e-02 -4.56443569e-02 -2.50976641e-02\n -1.96849705e-02  2.64800104e-03  1.71267322e-02  3.66680822e-03\n -3.79561777e-02  2.68794546e-02 -1.88232481e-02  3.85082185e-03\n  3.34321352e-02  7.54904704e-02 -1.10901874e-02 -6.34173809e-02\n -9.11540698e-03 -1.01252379e-02 -2.79027500e-02  2.23015545e-02\n  4.97734433e-03 -1.47390431e-02  1.10812111e-02 -1.25039507e-02\n  1.04035024e-02 -2.36704364e-02  2.55734067e-02 -9.42508846e-03\n  6.30583299e-02 -4.83821207e-03  4.77537815e-03 -9.71232925e-03\n  4.11562196e-03  2.15251068e-02 -6.77259928e-03  3.30955249e-02\n -3.32391453e-02 -2.33831956e-02 -2.20367544e-03  3.75477572e-02\n  9.29942061e-02  1.87200209e-02  1.06952313e-02  4.93246296e-02\n -1.01790955e-02 -1.42857412e-02  2.63678070e-02 -9.93224798e-03\n -3.61698990e-02  6.53472800e-02 -1.68035863e-02 -2.60760781e-02\n  3.24312806e-02  4.90553413e-03  3.30147384e-02 -5.46206317e-02\n  2.80149534e-02  3.20812058e-02  1.40388937e-02  1.11395569e-02\n -4.63175776e-02 -2.64037121e-02 -8.60375932e-02 -1.10273535e-02\n  2.41865722e-02 -9.27249179e-03 -2.78264516e-02  4.32432035e-02\n  4.97285619e-02 -2.46398741e-03  3.82748354e-02 -7.40991478e-02\n  6.34622623e-03 -1.06323974e-02  1.22795438e-02  5.46655130e-03\n -1.59149351e-02 -7.65676234e-03  9.61807836e-03  1.08657805e-02\n  1.05920042e-03  2.52592371e-02  2.01876418e-02 -5.45757503e-02\n  8.54990167e-03  1.43844802e-02 -3.57973836e-02 -2.23509241e-02\n -1.37875580e-02  3.42040948e-02  1.70638982e-02  3.92667138e-02\n -2.55419897e-01 -2.59414339e-02 -5.97371083e-02  3.17625481e-02\n  3.43611796e-02 -4.86962904e-02 -1.89848210e-01  5.14789355e-03\n -1.30066221e-02 -2.89664385e-02  4.75293746e-02  4.85616462e-02\n -4.98183247e-04 -1.34644121e-04  9.34879012e-03  1.21448997e-02\n  6.68283653e-03  3.36610302e-02 -3.04071306e-02  5.10301218e-02\n -2.73013396e-02  3.04340594e-02 -7.78243018e-03  3.29878096e-02\n -3.36610302e-03  3.94282867e-02 -1.22526150e-03 -3.05597273e-02\n -3.09501952e-02  1.40299174e-02 -3.34815047e-03 -7.33361645e-03\n -5.91536504e-03  6.14426005e-03  2.13859745e-02 -6.28339230e-04\n -1.84462445e-03  3.67578450e-03 -2.15565237e-02  1.73242102e-02\n -1.89937973e-02  8.07864725e-05  6.12630750e-03 -1.84013632e-02\n -3.06988595e-03  3.02635102e-02 -1.45999108e-02 -3.16548328e-02\n -3.15336531e-02  1.93348957e-02 -2.24317105e-02 -1.44742430e-02\n -2.65159155e-02  2.83201467e-03  4.44056310e-02 -1.08523161e-02\n -4.26373049e-03 -1.80423122e-03 -2.34460296e-02  1.40792869e-02\n  8.62620001e-03 -2.80912517e-02 -2.76918075e-02  1.70998033e-02\n -8.93588148e-03 -1.67227998e-02  5.21970375e-03  2.08339336e-02\n -8.45565079e-02 -2.05287403e-02  2.76199973e-02 -2.29119412e-02\n -1.62784742e-02  1.37920461e-02  1.73825560e-02 -3.74759470e-03\n  1.07984585e-02  3.72425638e-02  6.39559574e-03 -7.27078252e-04\n  2.47431013e-02 -4.97285619e-02 -9.47445797e-03 -1.22077336e-03\n  4.29694271e-02  6.98354173e-03  4.03663074e-02  3.73772079e-02\n -2.71622073e-02 -1.02015362e-02  2.45635758e-02 -4.82923580e-03\n -5.71788700e-03 -1.37561410e-02  3.51331393e-02 -5.12994100e-02\n -9.68540042e-03 -1.38369275e-02  2.47879826e-02  9.95468866e-03\n  2.92626556e-03  2.20412426e-02 -3.80145234e-03 -2.07441709e-02\n  3.29967859e-02 -1.68484676e-02 -2.41910604e-03 -2.40564162e-02\n -9.72579366e-03  1.28450491e-02 -3.25120670e-02  2.18213238e-02\n  1.39042495e-02  1.43755040e-02 -5.09852404e-02 -1.34644121e-05\n  1.44966837e-03 -1.46268397e-02  1.08523161e-02  1.02733464e-02\n  6.13977191e-03 -1.33118154e-02 -1.67901219e-02  2.33697312e-02\n  7.11818585e-03  9.29044433e-03 -1.10991637e-02  4.62278148e-02\n -1.49724262e-02 -3.64885567e-03  6.44047711e-02 -9.26800365e-03\n  1.43261345e-02  7.89912175e-02 -5.83906671e-02 -8.30305412e-02\n -1.35092935e-03  2.62241866e-02  1.31322899e-01  3.94956088e-03\n -5.83906671e-03  1.92989906e-02 -3.61205295e-02  6.04103289e-03\n  9.43406473e-03  3.66950111e-02  6.83992134e-03 -6.17118887e-02\n  4.72600864e-02  5.30049022e-02 -2.42090129e-02  1.64490234e-02\n -2.60626136e-02  2.09730659e-02  3.02949272e-03 -1.63592607e-02\n -3.94776562e-02 -3.07123240e-02 -2.31228837e-02 -3.63539126e-02\n  4.77537815e-02 -8.85509501e-03 -1.84372683e-02 -2.21040765e-02\n -4.71703237e-02 -2.35941381e-02 -8.21777951e-03 -6.33276181e-03\n -3.28800943e-02  0.00000000e+00  3.21126228e-02  6.75464673e-02\n -3.10040529e-02  5.69993445e-02  2.50482946e-02 -1.83834106e-02\n -1.45415650e-03 -2.30241447e-03 -1.37785817e-02  3.31000130e-02\n -1.63727251e-02  7.62085724e-03  3.19869550e-02 -1.70728745e-02\n  1.51743924e-02 -1.80153834e-02 -4.17396774e-04 -8.55887795e-03\n  3.40694507e-02 -9.51485120e-03  4.75742560e-02  4.46569667e-03\n -1.92900144e-02  4.30232847e-02  2.43975147e-02  3.64750923e-02\n -6.55716868e-03  1.48916398e-02  4.33733594e-02 -3.95315139e-02\n -1.03406685e-02  1.08433399e-02  1.77730239e-03 -4.07522872e-02\n  4.17845588e-02  3.05148459e-02 -6.08591426e-02  1.09375907e-02\n -6.67834839e-02  4.13312569e-02  2.17584899e-02  2.95633608e-02\n -1.36708664e-02  1.69113016e-02  3.85665643e-02 -4.23680167e-02\n  2.43077519e-02 -3.54338445e-02 -1.56815519e-02  2.13725101e-02\n  1.03900380e-01 -9.98610563e-03 -8.15045745e-03 -1.62650098e-02\n  1.14627028e-02  9.20068159e-04  1.44069209e-02  1.89354515e-02\n -9.31288502e-03  1.05920042e-03 -2.05511810e-02  2.57394678e-02\n -2.60042679e-02 -1.40029886e-03  2.06319674e-02  7.01944683e-03\n  1.90252143e-02 -1.87065565e-02  4.94143923e-02  3.53081766e-02\n -3.54473089e-02 -4.22782539e-03  1.07894822e-02  1.12203434e-03\n  8.88651197e-03 -3.20093956e-02 -2.00664621e-02  9.02115609e-03\n  2.90202962e-02  2.72115768e-02 -3.14708192e-02  2.63049731e-02\n  2.17405374e-02 -4.60482893e-03 -1.60585555e-02 -2.38364975e-02\n  2.68974072e-02  5.53836150e-02 -3.77452352e-03 -3.57031327e-02\n  1.90611194e-02  7.74652508e-03  6.48984662e-02  3.43746440e-02\n -5.17931051e-02 -2.30690260e-03  1.29707170e-02 -3.91500222e-02\n  5.12994100e-03  4.66317472e-02 -2.11570795e-02  2.57529322e-02\n -3.70002044e-02  1.14716791e-01 -5.28702581e-02  2.18078594e-02\n -2.28625717e-02 -1.33656731e-02  2.12244016e-02 -1.41421208e-02\n -2.29657989e-02  1.67227998e-02  7.10472144e-03  2.87644723e-02\n  8.35242363e-03 -2.16238458e-02 -6.84889761e-02  2.65607969e-02\n -8.03825401e-03  3.15067243e-03  4.20493589e-02 -3.15695582e-02\n  1.82667191e-03 -2.52592371e-02  2.41057858e-02 -2.22073037e-02\n -1.20192318e-02  4.32656441e-02 -3.44778712e-02 -5.25560885e-03\n  4.77986629e-02  1.80871936e-03 -1.13819163e-02 -8.83265432e-02\n  3.43656678e-02  1.67227998e-02 -1.07176720e-01 -2.62152103e-02\n -4.93695110e-04 -4.38491020e-03  5.52040895e-03 -2.02863809e-03\n -1.81365631e-01 -7.82282342e-03  5.49796827e-03  2.85131366e-02\n -4.77089001e-03  5.57426660e-03 -3.61025769e-02 -1.12382959e-01\n  9.09745443e-02 -8.12352862e-04 -7.98888450e-03  2.75796041e-02\n -3.73368147e-02 -2.32126464e-02 -4.59136452e-03  3.17535718e-02\n -1.10318416e-02  2.87689605e-02  3.34366233e-02 -3.68251670e-02\n -7.77794204e-03 -9.79311572e-03  3.73413028e-02  4.55545942e-02\n -1.24725337e-02 -1.46806973e-02 -5.58773101e-02  3.10713749e-02\n  1.01566548e-01  2.26650937e-02 -2.72878751e-02  2.21803748e-02\n -2.31363481e-02  6.96558918e-03  2.30420972e-02  3.32481216e-02\n -6.62897888e-03 -9.58666140e-03 -2.75077939e-02  1.95548145e-02\n  5.69544631e-02 -1.73152339e-02  3.86159338e-02  4.29963559e-03\n  1.52327382e-01 -2.02863809e-03  2.92312386e-02  4.22333726e-03\n  7.28424693e-02  5.83009043e-02  2.97518626e-02 -1.33926019e-02\n -1.53628942e-02 -5.47103944e-02 -5.85253112e-02 -2.53355354e-02\n  3.43836203e-02 -3.67578450e-03  2.83111705e-02  3.46888137e-02\n -3.23145890e-04  9.11989512e-03 -1.79480613e-02  2.05197640e-02\n -4.02855209e-02  1.16646690e-02  3.77272826e-02  2.95768252e-02\n  2.57574203e-02 -4.67215099e-02  1.31502425e-02 -1.10453060e-02\n -8.09211166e-03 -1.48691991e-02 -3.82434184e-02  1.34195307e-03\n -7.23936556e-02  2.19918731e-03  2.32754803e-02 -3.46125153e-02\n  6.10835495e-03 -2.38364975e-02  3.77811403e-02 -9.11091884e-04\n  3.07437409e-03 -8.99422727e-03 -1.54032874e-02 -3.73009096e-02\n -1.16601809e-02  3.80728692e-02  3.06315375e-02  1.80243596e-02\n  1.33207917e-02 -3.09905885e-02  4.57341197e-03 -4.14569248e-02\n -4.51955432e-03  2.74584244e-02 -2.01248079e-02 -4.49262550e-02\n -2.38993314e-02  3.63000550e-02 -1.60226504e-02  6.09489053e-02\n  4.33733594e-02  3.09277545e-02 -2.53983693e-02 -5.45757503e-02\n  1.16691571e-03  9.30839688e-02  6.77708741e-03  2.15520356e-02\n -3.48728273e-03 -2.11121981e-02  8.05171842e-03  7.74652508e-03\n -1.08612924e-02  2.87689605e-02  2.09865303e-02 -2.53938812e-02\n  1.52147856e-03 -3.65693432e-02 -3.80279878e-02 -2.03851199e-02\n  9.29044433e-03 -4.38805190e-02 -5.69993445e-04  4.15601520e-03\n -1.27822152e-02 -4.08240974e-02  3.90737239e-02  7.02393497e-02\n  1.74139730e-03  2.38005924e-02 -5.99166338e-03 -4.80679511e-02\n  6.13079563e-02  1.59194232e-02  5.31844277e-02 -2.22387206e-02\n  5.23765630e-03  1.13101061e-03  6.62000261e-03 -7.43235547e-03\n -3.53979394e-02 -8.17289813e-03 -1.40927513e-03 -2.05960623e-02\n -1.12697129e-02 -3.84678253e-02  3.14483785e-02  4.41139021e-02\n  5.90190063e-02  2.08563743e-02 -1.71042915e-02 -1.33387442e-01\n -1.61752470e-02  2.55823830e-03  1.46447922e-02  3.98905649e-02\n -3.33603250e-02 -2.08563743e-02 -1.18531708e-02 -2.95902896e-02\n  4.72152050e-02  1.75979866e-02  7.65227420e-03 -2.66999292e-02\n -4.05772499e-02 -3.63045431e-02  2.96127303e-02 -2.14173915e-02\n  5.78520906e-03 -1.33297680e-03  2.04838589e-02 -6.49433476e-03\n  2.10179473e-02 -7.72857253e-03  4.48723973e-02  5.77174464e-02\n -2.76738550e-02  4.32207628e-03  7.33361645e-02 -3.92442731e-02\n  2.21310053e-02 -1.27014287e-03 -1.39581072e-02 -5.01324943e-03\n -2.03222860e-02  5.94229386e-02 -2.25753309e-03 -5.72237513e-02\n -7.85424038e-03 -3.12912937e-02 -1.22077336e-02  1.31547306e-02\n  6.85787389e-02  1.54975383e-02 -2.26875344e-02 -1.42139310e-02\n  4.90553413e-02  3.77003538e-04  3.86473508e-02 -2.50527827e-02\n  1.35003172e-02  7.40093851e-02  1.33207917e-02 -1.00534277e-03\n -2.69288242e-03  4.86065276e-03  3.08065748e-02  1.81455393e-02\n  1.89085227e-02 -3.05058696e-02  1.29617407e-02  1.94022178e-02\n -1.51743924e-02  1.52910840e-02  4.07702398e-02  3.07751579e-02\n  4.76640188e-02  3.41188202e-02 -4.70356795e-03  1.61303657e-02\n  2.86163638e-02 -1.25578083e-02 -1.79974308e-02  3.07078358e-02\n  5.18828679e-02  1.58072198e-02 -1.78223935e-02 -9.49241052e-03\n -1.20596251e-02  2.33203617e-02  7.15409095e-03  4.00341852e-02\n  2.91055708e-02  2.72788989e-02  1.70504338e-02 -7.75101322e-03\n  1.71312203e-02  1.61572945e-02  2.29972158e-02 -6.34622623e-03\n  2.76424380e-02  2.10448761e-02 -3.46933018e-03  2.12199134e-02\n -6.41803642e-04 -2.39846061e-02  1.60585555e-02  1.45774701e-02\n  3.99578869e-02  1.53987993e-02  4.64073403e-02  5.86599553e-02\n -4.59136452e-02  3.45990509e-02  3.22248262e-02  2.45052300e-03\n -4.04515820e-02  2.11391270e-03  1.76249154e-02 -5.15238169e-03\n -2.13635338e-03 -7.23487742e-03  6.70078908e-03 -9.39815963e-03\n  2.15655000e-02  2.22836020e-02  1.65163455e-02  3.78215335e-02\n  4.67663913e-03  6.29685672e-03  7.89014548e-03  5.64158866e-02\n -2.68345733e-02  5.49796827e-02 -2.48193996e-03  3.29922977e-02\n  2.60760781e-02  4.30412373e-02  1.08927094e-02  2.22162799e-03\n  1.85853768e-02  3.78439742e-02  2.76559024e-02 -3.28441892e-02\n  5.43962248e-02  2.85535299e-02 -1.98689841e-02  5.78072092e-03\n  6.66039584e-02  8.07415911e-03  2.40878332e-02 -4.98632061e-03\n -4.54199501e-02  3.00031982e-02  1.28450491e-02  1.15973469e-02\n  3.62461973e-02  2.47879826e-02 -8.58131863e-03  2.95498964e-02\n -1.78672748e-02  9.60012581e-03  1.70594101e-02  1.87424616e-02]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Get Data Tensor"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train))\ntrain_data_tensor =  torch.from_numpy(train.astype(np.float32))\ntrain_target_tensor = torch.from_numpy(target.astype(np.float32))","execution_count":178,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"max_val {}, min_val {}\",train_data_tensor.max() , train_data_tensor.min() )\n#Value range b/w -1 to 1 implies that it has been normalized","execution_count":184,"outputs":[{"output_type":"stream","text":"max_val {}, min_val {} tensor(1.) tensor(-0.4424)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_target_tensor)","execution_count":185,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sum(train_target_tensor)))","execution_count":186,"outputs":[{"output_type":"stream","text":"tensor(16844.)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"type of train_data_tensor: {}\",type(train_data_tensor) )\ntrain_set = torch.utils.data.TensorDataset(train_data_tensor , train_target_tensor)\nprint(train_set)","execution_count":187,"outputs":[{"output_type":"stream","text":"type of train_data_tensor: {} <class 'torch.Tensor'>\n<torch.utils.data.dataset.TensorDataset object at 0x7f726352a990>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Get dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 300\ntrain_loader = torch.utils.data.DataLoader(train_set , batch_size = batch_size)\nprint(train_loader)","execution_count":189,"outputs":[{"output_type":"stream","text":"<torch.utils.data.dataloader.DataLoader object at 0x7f72634d0510>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Now take Instance of Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultiLabelClassifer(876 , 1024  , 206 )\n#(self , input_feature , hidden_dim , output_dim):\n#model = ModelTWO(876 , 206 , 1024)","execution_count":198,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyser Sample Formed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(train_loader))\nfeature, label = batch","execution_count":199,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(batch[0].shape)\nprint(batch[0][0].shape) #THis is one extracted Feature out of 100","execution_count":200,"outputs":[{"output_type":"stream","text":"torch.Size([300, 876])\ntorch.Size([876])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(feature)","execution_count":201,"outputs":[{"output_type":"stream","text":"tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.6001e-03,\n          1.7059e-02,  1.8742e-02],\n        [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  ...,  5.6319e-03,\n          2.7579e-02,  3.3451e-02],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ..., -7.6819e-03,\n         -4.9456e-02,  2.4345e-02],\n        ...,\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.1164e-03,\n          3.6492e-02,  2.0576e-02],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  8.3413e-04,\n         -2.9741e-02,  5.5212e-03],\n        [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.4815e-02,\n         -2.0204e-02, -3.4210e-02]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(label[0]))\nprint(label[0])   #label for 1st feature data","execution_count":202,"outputs":[{"output_type":"stream","text":"tensor(1.)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Now train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.optim as optim\nlearning_rate = [ 1 , .01 , .001 , .0001]\n\nfor lr in learning_rate:\n    optimizer = optim.Adam(model.parameters() , lr = lr)\n    criterion = torch.nn.BCELoss()\n\n    for epoch in range(10):\n        model.train()\n        total_loss = 0\n        total_correct = 0\n\n        for batch in train_loader:\n            features , labels  = batch\n            optimizer.zero_grad()\n\n            preds = model(features)\n\n            #print(sum(preds>1))\n            loss = criterion(preds , labels)\n\n\n            loss.backward()\n            optimizer.step() #update weights\n\n            total_loss += loss.data.item()\n            #print(\"total_loss: {}\".format(total_loss))\n            #total_correct  += get_num_correct(preds,labels)\n        print(\" epoch: {} , learning_rate:{} , total_loss: {}\".format(epoch, lr, total_loss))","execution_count":210,"outputs":[{"output_type":"stream","text":"learning_rate:1 , epoch: 0 total_loss: 1139.9748458862305\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-210-3f9f0aab86a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}