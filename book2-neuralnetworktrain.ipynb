{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Read Input Files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa', 'train_features.csv'))\ntest_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa' , 'test_features.csv'))\n\ntrain_target = pd.read_csv(os.path.join('/kaggle/input/lish-moa','train_targets_scored.csv'))\nsample = pd.read_csv(os.path.join('/kaggle/input/lish-moa' , 'sample_submission.csv'))","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data to train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data , train_target, test_target  = train_test_split(train_data,train_target, test_size=0.2)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Print Shape of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_data.shap: {}  , train_target.shap: {}  , test_data.shap: {} \".format(train_data.shape , train_target.shape , test_data.shape))","execution_count":4,"outputs":[{"output_type":"stream","text":"train_data.shap: (19051, 876)  , train_target.shap: (19051, 207)  , test_data.shap: (4763, 876) \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGTypeFeature = sum(train_data.columns.to_series().str.contains('g-') == True )\nNCTypeFeature = sum(train_data.columns.to_series().str.contains('c-') == True )\n\nprint(\"NGTypeFeature = {} \\nNCTypeFeature = {}\".format(NGTypeFeature , NCTypeFeature))","execution_count":5,"outputs":[{"output_type":"stream","text":"NGTypeFeature = 772 \nNCTypeFeature = 100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Define a Neural Network To Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\nclass MultiLabelClassifer(nn.Module):\n    #TO-DO: Documentation\n    def __init__(self , input_feature , hidden_dim , output_dim):\n        super(MultiLabelClassifer , self).__init__()\n        \n        #nn.utils.weight_norm\n        self.fc1 = (nn.Linear(in_features = input_feature , out_features = hidden_dim))\n        #TO-DO: As close features are related here, convoluation layer will help here\n        #TO-DO: Later add and play with Convolutatin layer, to see accurracy difference\n        self.fc2 =  (nn.Linear( in_features = hidden_dim , out_features  = hidden_dim))\n        self.fc3 = (nn.Linear( in_features = hidden_dim , out_features  = hidden_dim))\n        self.fc4 =  (nn.Linear(in_features = hidden_dim , out_features = hidden_dim))\n        self.fc5 = (nn.Linear(in_features = hidden_dim , out_features = output_dim))\n        self.drop = nn.Dropout(0.4) # dropout with 30% prob\n        \n        self.out = (nn.Linear(output_dim,output_dim)) #Above using weight normalization for final layer\n        #Query: Why use weight normalization. Checkout this awesome stackoverlfow post:\n        #link: https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn\n        \n        self.sig = nn.Sigmoid()\n        #self.F_Sig = F.sigmoid() #signout output range from [0,1]\n        self.Log_Sig = nn.LogSigmoid() #log of range 0-1 is whole number?\n        self.soft = nn.Softmax()  #sum of all output is '1'. So its used only in multi-class problem.\n        #self.linear3 = nn.utils.weight_norm(nn.Linear(hidden_size,output_size))\n        #TO-DO: Define output layer so that it be as per Question requirement\n    def forward(self, x):\n        \n        # (1) input layer\n        \n        t = x\n        \n        \n        # (2) Hidden Linear Layer\n        \n        t = self.fc1(t)\n        t = F.relu(t)\n        #3\n        \n        t = self.fc2(t)\n        t = F.relu(t)\n        #4\n        \n        t= self.fc3(t)\n        t = F.relu(t)\n        t = self.drop(t)\n        \n        #5\n        \n        t = self.fc4(t)\n        t = F.relu(t)\n        #6\n        \n        t = self.drop(t)\n        \n        t = self.fc5(t)\n        #t = F.relu(t)\n        #t = self.F_Sig(t)\n        #t = F.sigmoid(t)\n        t = self.Log_Sig(t) #Works, if no activation is used after fc5\n        \n        # (7) Output Layer\n        \n        #t = self.out(t) #This working okay. but why? \n        \n        #t = self.sig(t)  #Using Sigmmoid ALSO as last layer is causing training loss to not decrease\n        #t=self.soft(t) #==> Using softmax as last layer is causing training loss to not decrease\n        \n        #t = self.Log_Sig(t)\n        \n        return t\n        ","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Make Copy of data to Work ON"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nn = train_data.copy()\ntrain_data.head()\n\ntest_nn = test_data.copy()\ntrain_nn.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"             sig_id      cp_type  cp_time cp_dose     g-0     g-1     g-2  \\\n12696  id_88cbf9e6f       trt_cp       24      D1 -0.0363 -0.7052  2.4370   \n15178  id_a31fe4c72       trt_cp       24      D2 -0.5555 -0.2524 -0.0457   \n23144  id_f89cb0273       trt_cp       48      D2  4.1270  0.3536  3.1270   \n6605   id_46a556983       trt_cp       48      D1 -0.7084 -1.6320 -0.3802   \n7014   id_4b33dbf64  ctl_vehicle       24      D2  0.2139 -1.2400  0.3305   \n\n          g-3     g-4     g-5  ...     c-90     c-91    c-92     c-93  \\\n12696  0.4958 -0.0288  0.9325  ...   0.8354   0.6232  1.3220   0.7226   \n15178 -1.0740 -0.2493 -0.1253  ...   0.6307  -0.1247  1.0530   0.8649   \n23144  2.5220  1.0280 -1.9550  ... -10.0000 -10.0000 -8.8670 -10.0000   \n6605   0.6172 -1.8590  0.5889  ...   1.1510  -0.8152  0.7311   0.7782   \n7014  -0.4977 -1.5110  0.0809  ...  -0.1928  -1.2300 -0.0741   0.4578   \n\n          c-94    c-95     c-96    c-97    c-98    c-99  \n12696   1.5900  0.9043   0.5915  0.6916  0.4090  1.3070  \n15178  -1.1550 -0.0001  -0.3790 -0.3946  0.6762 -0.2142  \n23144 -10.0000 -8.8740 -10.0000 -7.2720 -8.8470 -7.6540  \n6605    0.7926  0.3287   0.8066  0.6009 -0.0479  0.1848  \n7014    0.8259  0.9920  -0.4301  0.3867 -1.0180  0.3901  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12696</th>\n      <td>id_88cbf9e6f</td>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>-0.0363</td>\n      <td>-0.7052</td>\n      <td>2.4370</td>\n      <td>0.4958</td>\n      <td>-0.0288</td>\n      <td>0.9325</td>\n      <td>...</td>\n      <td>0.8354</td>\n      <td>0.6232</td>\n      <td>1.3220</td>\n      <td>0.7226</td>\n      <td>1.5900</td>\n      <td>0.9043</td>\n      <td>0.5915</td>\n      <td>0.6916</td>\n      <td>0.4090</td>\n      <td>1.3070</td>\n    </tr>\n    <tr>\n      <th>15178</th>\n      <td>id_a31fe4c72</td>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>-0.5555</td>\n      <td>-0.2524</td>\n      <td>-0.0457</td>\n      <td>-1.0740</td>\n      <td>-0.2493</td>\n      <td>-0.1253</td>\n      <td>...</td>\n      <td>0.6307</td>\n      <td>-0.1247</td>\n      <td>1.0530</td>\n      <td>0.8649</td>\n      <td>-1.1550</td>\n      <td>-0.0001</td>\n      <td>-0.3790</td>\n      <td>-0.3946</td>\n      <td>0.6762</td>\n      <td>-0.2142</td>\n    </tr>\n    <tr>\n      <th>23144</th>\n      <td>id_f89cb0273</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D2</td>\n      <td>4.1270</td>\n      <td>0.3536</td>\n      <td>3.1270</td>\n      <td>2.5220</td>\n      <td>1.0280</td>\n      <td>-1.9550</td>\n      <td>...</td>\n      <td>-10.0000</td>\n      <td>-10.0000</td>\n      <td>-8.8670</td>\n      <td>-10.0000</td>\n      <td>-10.0000</td>\n      <td>-8.8740</td>\n      <td>-10.0000</td>\n      <td>-7.2720</td>\n      <td>-8.8470</td>\n      <td>-7.6540</td>\n    </tr>\n    <tr>\n      <th>6605</th>\n      <td>id_46a556983</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.7084</td>\n      <td>-1.6320</td>\n      <td>-0.3802</td>\n      <td>0.6172</td>\n      <td>-1.8590</td>\n      <td>0.5889</td>\n      <td>...</td>\n      <td>1.1510</td>\n      <td>-0.8152</td>\n      <td>0.7311</td>\n      <td>0.7782</td>\n      <td>0.7926</td>\n      <td>0.3287</td>\n      <td>0.8066</td>\n      <td>0.6009</td>\n      <td>-0.0479</td>\n      <td>0.1848</td>\n    </tr>\n    <tr>\n      <th>7014</th>\n      <td>id_4b33dbf64</td>\n      <td>ctl_vehicle</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.2139</td>\n      <td>-1.2400</td>\n      <td>0.3305</td>\n      <td>-0.4977</td>\n      <td>-1.5110</td>\n      <td>0.0809</td>\n      <td>...</td>\n      <td>-0.1928</td>\n      <td>-1.2300</td>\n      <td>-0.0741</td>\n      <td>0.4578</td>\n      <td>0.8259</td>\n      <td>0.9920</td>\n      <td>-0.4301</td>\n      <td>0.3867</td>\n      <td>-1.0180</td>\n      <td>0.3901</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## One hot coding of input feature and Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data\nimport torch\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,Normalizer\n\nignore_columns = ['sig_id','cp_type']\n\ntrain_columns = [x for x in train_data.columns if x not in ignore_columns]\ntrain = train_nn[train_columns]  #Extract Column Except ignore_columns\ntest = test_nn[train_columns]\n\n#Store Original Index and Column\nidx = train.index\ncol = train.columns\n\ntrain.head()\n\n\n\n","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"       cp_time cp_dose     g-0     g-1     g-2     g-3     g-4     g-5  \\\n12696       24      D1 -0.0363 -0.7052  2.4370  0.4958 -0.0288  0.9325   \n15178       24      D2 -0.5555 -0.2524 -0.0457 -1.0740 -0.2493 -0.1253   \n23144       48      D2  4.1270  0.3536  3.1270  2.5220  1.0280 -1.9550   \n6605        48      D1 -0.7084 -1.6320 -0.3802  0.6172 -1.8590  0.5889   \n7014        24      D2  0.2139 -1.2400  0.3305 -0.4977 -1.5110  0.0809   \n\n          g-6     g-7  ...     c-90     c-91    c-92     c-93     c-94  \\\n12696  2.0920  1.2900  ...   0.8354   0.6232  1.3220   0.7226   1.5900   \n15178  1.7780  0.0968  ...   0.6307  -0.1247  1.0530   0.8649  -1.1550   \n23144  0.0772 -2.0710  ... -10.0000 -10.0000 -8.8670 -10.0000 -10.0000   \n6605   0.1296 -1.5620  ...   1.1510  -0.8152  0.7311   0.7782   0.7926   \n7014  -1.3840 -1.8560  ...  -0.1928  -1.2300 -0.0741   0.4578   0.8259   \n\n         c-95     c-96    c-97    c-98    c-99  \n12696  0.9043   0.5915  0.6916  0.4090  1.3070  \n15178 -0.0001  -0.3790 -0.3946  0.6762 -0.2142  \n23144 -8.8740 -10.0000 -7.2720 -8.8470 -7.6540  \n6605   0.3287   0.8066  0.6009 -0.0479  0.1848  \n7014   0.9920  -0.4301  0.3867 -1.0180  0.3901  \n\n[5 rows x 874 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>g-6</th>\n      <th>g-7</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12696</th>\n      <td>24</td>\n      <td>D1</td>\n      <td>-0.0363</td>\n      <td>-0.7052</td>\n      <td>2.4370</td>\n      <td>0.4958</td>\n      <td>-0.0288</td>\n      <td>0.9325</td>\n      <td>2.0920</td>\n      <td>1.2900</td>\n      <td>...</td>\n      <td>0.8354</td>\n      <td>0.6232</td>\n      <td>1.3220</td>\n      <td>0.7226</td>\n      <td>1.5900</td>\n      <td>0.9043</td>\n      <td>0.5915</td>\n      <td>0.6916</td>\n      <td>0.4090</td>\n      <td>1.3070</td>\n    </tr>\n    <tr>\n      <th>15178</th>\n      <td>24</td>\n      <td>D2</td>\n      <td>-0.5555</td>\n      <td>-0.2524</td>\n      <td>-0.0457</td>\n      <td>-1.0740</td>\n      <td>-0.2493</td>\n      <td>-0.1253</td>\n      <td>1.7780</td>\n      <td>0.0968</td>\n      <td>...</td>\n      <td>0.6307</td>\n      <td>-0.1247</td>\n      <td>1.0530</td>\n      <td>0.8649</td>\n      <td>-1.1550</td>\n      <td>-0.0001</td>\n      <td>-0.3790</td>\n      <td>-0.3946</td>\n      <td>0.6762</td>\n      <td>-0.2142</td>\n    </tr>\n    <tr>\n      <th>23144</th>\n      <td>48</td>\n      <td>D2</td>\n      <td>4.1270</td>\n      <td>0.3536</td>\n      <td>3.1270</td>\n      <td>2.5220</td>\n      <td>1.0280</td>\n      <td>-1.9550</td>\n      <td>0.0772</td>\n      <td>-2.0710</td>\n      <td>...</td>\n      <td>-10.0000</td>\n      <td>-10.0000</td>\n      <td>-8.8670</td>\n      <td>-10.0000</td>\n      <td>-10.0000</td>\n      <td>-8.8740</td>\n      <td>-10.0000</td>\n      <td>-7.2720</td>\n      <td>-8.8470</td>\n      <td>-7.6540</td>\n    </tr>\n    <tr>\n      <th>6605</th>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.7084</td>\n      <td>-1.6320</td>\n      <td>-0.3802</td>\n      <td>0.6172</td>\n      <td>-1.8590</td>\n      <td>0.5889</td>\n      <td>0.1296</td>\n      <td>-1.5620</td>\n      <td>...</td>\n      <td>1.1510</td>\n      <td>-0.8152</td>\n      <td>0.7311</td>\n      <td>0.7782</td>\n      <td>0.7926</td>\n      <td>0.3287</td>\n      <td>0.8066</td>\n      <td>0.6009</td>\n      <td>-0.0479</td>\n      <td>0.1848</td>\n    </tr>\n    <tr>\n      <th>7014</th>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.2139</td>\n      <td>-1.2400</td>\n      <td>0.3305</td>\n      <td>-0.4977</td>\n      <td>-1.5110</td>\n      <td>0.0809</td>\n      <td>-1.3840</td>\n      <td>-1.8560</td>\n      <td>...</td>\n      <td>-0.1928</td>\n      <td>-1.2300</td>\n      <td>-0.0741</td>\n      <td>0.4578</td>\n      <td>0.8259</td>\n      <td>0.9920</td>\n      <td>-0.4301</td>\n      <td>0.3867</td>\n      <td>-1.0180</td>\n      <td>0.3901</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 874 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntarget = train_target.iloc[:,1:].values\n#test_target = test_target[:,1:]\n#https://stackoverflow.com/questions/54160370/how-to-use-sklearn-column-transformer/54160620\ntransform = ColumnTransformer([('o',OneHotEncoder(),[0,1]),('s',Normalizer(),list(range(3,train.shape[1])))])\nprint(\"size before transform train.shape:{} test.shape{}\".format(train.shape,test.shape))\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)\n\nprint(\"size AFTER transform train.shape:{} test.shape{}\".format(train.shape,test.shape))","execution_count":9,"outputs":[{"output_type":"stream","text":"size before transform train.shape:(19051, 874) test.shape(4763, 874)\nsize AFTER transform train.shape:(19051, 876) test.shape(4763, 876)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Check Data After Transformation "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame(data = train,index = idx ) #Check if encoding is successful or not\ntrain_df.head()\n","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"       0    1    2    3    4         5         6         7         8    \\\n12696  1.0  0.0  0.0  1.0  0.0 -0.022241  0.076860  0.015637 -0.000908   \n15178  1.0  0.0  0.0  0.0  1.0 -0.010850 -0.001965 -0.046170 -0.010717   \n23144  0.0  1.0  0.0  0.0  1.0  0.002474  0.021879  0.017646  0.007193   \n6605   0.0  1.0  0.0  1.0  0.0 -0.057470 -0.013389  0.021734 -0.065464   \n7014   1.0  0.0  0.0  0.0  1.0 -0.042939  0.011445 -0.017235 -0.052324   \n\n            9    ...       866       867       868       869       870  \\\n12696  0.029410  ...  0.026348  0.019655  0.041694  0.022790  0.050147   \n15178 -0.005386  ...  0.027113 -0.005361  0.045267  0.037181 -0.049652   \n23144 -0.013679  ... -0.069969 -0.069969 -0.062042 -0.069969 -0.069969   \n6605   0.020738  ...  0.040532 -0.028707  0.025745  0.027404  0.027911   \n7014   0.002801  ... -0.006676 -0.042593 -0.002566  0.015853  0.028600   \n\n            871       872       873       874       875  \n12696  0.028521  0.018655  0.021812  0.012899  0.041221  \n15178 -0.000004 -0.016293 -0.016963  0.029069 -0.009208  \n23144 -0.062090 -0.069969 -0.050881 -0.061902 -0.053554  \n6605   0.011575  0.028404  0.021160 -0.001687  0.006508  \n7014   0.034352 -0.014894  0.013391 -0.035252  0.013509  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>866</th>\n      <th>867</th>\n      <th>868</th>\n      <th>869</th>\n      <th>870</th>\n      <th>871</th>\n      <th>872</th>\n      <th>873</th>\n      <th>874</th>\n      <th>875</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12696</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.022241</td>\n      <td>0.076860</td>\n      <td>0.015637</td>\n      <td>-0.000908</td>\n      <td>0.029410</td>\n      <td>...</td>\n      <td>0.026348</td>\n      <td>0.019655</td>\n      <td>0.041694</td>\n      <td>0.022790</td>\n      <td>0.050147</td>\n      <td>0.028521</td>\n      <td>0.018655</td>\n      <td>0.021812</td>\n      <td>0.012899</td>\n      <td>0.041221</td>\n    </tr>\n    <tr>\n      <th>15178</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.010850</td>\n      <td>-0.001965</td>\n      <td>-0.046170</td>\n      <td>-0.010717</td>\n      <td>-0.005386</td>\n      <td>...</td>\n      <td>0.027113</td>\n      <td>-0.005361</td>\n      <td>0.045267</td>\n      <td>0.037181</td>\n      <td>-0.049652</td>\n      <td>-0.000004</td>\n      <td>-0.016293</td>\n      <td>-0.016963</td>\n      <td>0.029069</td>\n      <td>-0.009208</td>\n    </tr>\n    <tr>\n      <th>23144</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.002474</td>\n      <td>0.021879</td>\n      <td>0.017646</td>\n      <td>0.007193</td>\n      <td>-0.013679</td>\n      <td>...</td>\n      <td>-0.069969</td>\n      <td>-0.069969</td>\n      <td>-0.062042</td>\n      <td>-0.069969</td>\n      <td>-0.069969</td>\n      <td>-0.062090</td>\n      <td>-0.069969</td>\n      <td>-0.050881</td>\n      <td>-0.061902</td>\n      <td>-0.053554</td>\n    </tr>\n    <tr>\n      <th>6605</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.057470</td>\n      <td>-0.013389</td>\n      <td>0.021734</td>\n      <td>-0.065464</td>\n      <td>0.020738</td>\n      <td>...</td>\n      <td>0.040532</td>\n      <td>-0.028707</td>\n      <td>0.025745</td>\n      <td>0.027404</td>\n      <td>0.027911</td>\n      <td>0.011575</td>\n      <td>0.028404</td>\n      <td>0.021160</td>\n      <td>-0.001687</td>\n      <td>0.006508</td>\n    </tr>\n    <tr>\n      <th>7014</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.042939</td>\n      <td>0.011445</td>\n      <td>-0.017235</td>\n      <td>-0.052324</td>\n      <td>0.002801</td>\n      <td>...</td>\n      <td>-0.006676</td>\n      <td>-0.042593</td>\n      <td>-0.002566</td>\n      <td>0.015853</td>\n      <td>0.028600</td>\n      <td>0.034352</td>\n      <td>-0.014894</td>\n      <td>0.013391</td>\n      <td>-0.035252</td>\n      <td>0.013509</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Observe that features are now one hot coded , and rest of the value are normalized"},{"metadata":{},"cell_type":"markdown","source":"## Get Data Tensor"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train))\ntrain_data_tensor =  torch.from_numpy(train.astype(np.float32))\ntrain_target_tensor = torch.from_numpy(target.astype(np.float32))","execution_count":11,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"max_val {}, min_val {}\",train_data_tensor.max() , train_data_tensor.min() )\n#Value range b/w -1 to 1 implies that it has been normalized","execution_count":12,"outputs":[{"output_type":"stream","text":"max_val {}, min_val {} tensor(1.) tensor(-0.4424)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_target_tensor[0]) #Checking expected label for first training data","execution_count":13,"outputs":[{"output_type":"stream","text":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sum(train_target_tensor))) #Here, it should be whole number. As output lables are not float","execution_count":14,"outputs":[{"output_type":"stream","text":"tensor(13440.)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Combine features and expected label in one set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"type of train_data_tensor: {}\",type(train_data_tensor) )\ntrain_set = torch.utils.data.TensorDataset(train_data_tensor , train_target_tensor)\nprint(train_set)","execution_count":15,"outputs":[{"output_type":"stream","text":"type of train_data_tensor: {} <class 'torch.Tensor'>\n<torch.utils.data.dataset.TensorDataset object at 0x7f5c18053b10>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Get dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 120\ntrain_loader = torch.utils.data.DataLoader(train_set , batch_size = batch_size, shuffle=True)\nprint(train_loader)","execution_count":16,"outputs":[{"output_type":"stream","text":"<torch.utils.data.dataloader.DataLoader object at 0x7f5c18058290>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Now take Instance of Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_features = 876\noutput_featuers = 206\nhidden_dimension = 4000\n\n# def __init__(self , input_feature , hidden_dim , output_dim):\nmodel = MultiLabelClassifer(input_features , hidden_dimension  ,output_featuers )","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyser Sample Formed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(train_loader))\nfeature, label = batch","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(batch[0].shape)\nprint(batch[0][0].shape) #THis is one extracted Feature out of 100","execution_count":19,"outputs":[{"output_type":"stream","text":"torch.Size([120, 876])\ntorch.Size([876])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(feature[0]) #input features for first input data point","execution_count":20,"outputs":[{"output_type":"stream","text":"tensor([ 1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n        -1.5511e-02, -1.9007e-02,  2.8665e-02,  2.3071e-02,  3.6160e-03,\n         6.6140e-03,  6.6232e-03, -2.7438e-02,  1.8108e-02,  8.5880e-03,\n        -3.9024e-02,  2.4044e-02, -1.4252e-02, -3.3388e-02,  2.5699e-02,\n         4.3775e-02,  6.0928e-03,  5.9729e-02,  2.6779e-02, -1.5101e-02,\n         2.0317e-02,  1.5802e-02,  0.0000e+00,  4.2811e-02,  4.3406e-02,\n        -2.5054e-02, -2.7531e-02, -3.9804e-03, -9.4090e-04,  9.0815e-02,\n        -3.8466e-03,  4.9397e-02, -5.7653e-04, -3.5690e-02,  8.6664e-03,\n        -2.8785e-02,  9.9533e-03, -2.5570e-02, -4.0002e-02,  5.2949e-02,\n         4.9443e-02,  2.4514e-02, -2.0423e-02, -6.4110e-03,  2.2605e-02,\n        -5.7653e-02,  2.6322e-02,  1.1992e-03, -2.2379e-02,  1.1162e-02,\n        -7.7532e-03, -5.9175e-03,  1.2914e-03,  3.5487e-02,  4.4854e-02,\n         1.3080e-02,  2.5188e-02,  1.2070e-02,  1.1544e-02,  4.0090e-02,\n         1.0576e-02, -2.0423e-02,  1.9856e-02,  3.7493e-02, -2.7558e-02,\n         4.1889e-02, -1.1033e-01,  5.1242e-02,  8.9155e-03,  3.6114e-02,\n        -2.9597e-02,  5.2026e-02, -6.7154e-02, -6.4110e-04,  5.6085e-02,\n        -8.9755e-03, -2.0368e-02,  4.3286e-02, -2.6359e-02,  1.8643e-02,\n        -1.8620e-02,  1.6909e-01,  3.6677e-02, -1.9270e-02, -2.0133e-02,\n        -3.1105e-02,  9.0400e-03, -5.9037e-03, -2.7198e-02,  2.3767e-02,\n        -6.3834e-02,  4.4757e-02,  3.9721e-02,  1.0405e-02, -7.8870e-04,\n        -3.2350e-02, -1.7665e-03,  4.4531e-02,  3.1405e-02, -5.4471e-02,\n         2.2185e-03,  1.0165e-02, -1.3090e-02, -2.4906e-03,  2.6018e-02,\n         1.2361e-03,  3.0335e-02, -1.6807e-02,  8.1729e-03,  0.0000e+00,\n         2.1327e-02, -1.6687e-02, -3.4384e-02,  1.2960e-02, -3.7645e-02,\n         1.8855e-02,  2.4233e-02, -1.2458e-02,  1.4340e-02, -3.4361e-02,\n         3.0588e-02, -3.8738e-02, -3.4297e-02,  1.1392e-02,  2.4911e-02,\n        -1.8172e-02, -1.5322e-02, -2.0672e-02, -3.1811e-02, -9.7780e-03,\n        -3.1599e-02,  4.5569e-03, -9.8610e-03,  5.9590e-02, -1.7158e-02,\n         4.9582e-02,  4.7691e-02,  5.7100e-02, -3.8226e-02,  3.6345e-03,\n        -2.0953e-02,  2.3301e-02, -1.2960e-02,  8.5557e-03,  3.6354e-02,\n        -3.0478e-02,  1.1014e-02, -1.9570e-02,  1.8431e-02,  1.3223e-02,\n        -9.9625e-03,  3.5205e-02, -1.6115e-02, -3.5883e-02,  5.2119e-04,\n        -2.6516e-02, -2.2402e-02, -3.0533e-03, -2.8827e-03,  5.0550e-02,\n         6.4110e-03,  2.0086e-02, -4.4125e-02, -6.5402e-02,  3.9306e-02,\n         1.9925e-02, -4.0925e-02, -1.4123e-01, -1.1046e-02, -1.1895e-02,\n         1.5345e-02,  6.8538e-02, -1.7859e-02, -5.1565e-02,  1.7070e-02,\n         3.5136e-02,  1.2684e-02,  3.8240e-02,  4.7598e-02, -7.1767e-02,\n        -1.8772e-02,  6.9553e-02,  5.7515e-03, -3.5754e-02,  2.3984e-02,\n         3.0575e-02, -4.8014e-02,  1.1033e-02,  1.2448e-02,  3.1668e-02,\n        -4.1174e-02,  1.2199e-02,  2.2960e-02,  1.7005e-02,  2.9431e-02,\n         4.2391e-02, -3.3683e-02, -5.3133e-02,  2.5736e-03, -2.2669e-02,\n         2.0105e-02,  1.1632e-02,  2.3232e-02,  1.9653e-02, -1.7504e-02,\n         2.1567e-02, -4.3447e-02,  7.1951e-02,  5.6731e-04,  2.5921e-02,\n         3.6160e-03,  1.1194e-02, -7.0660e-03,  1.0820e-02, -5.9037e-02,\n        -1.1457e-02, -5.4747e-02,  2.9574e-02, -3.4869e-03,  8.3113e-03,\n         2.5058e-02,  3.6423e-02,  3.4979e-02,  4.5200e-04, -1.0770e-02,\n        -4.6722e-02,  3.5099e-03,  2.7572e-02, -2.0764e-02,  1.8634e-03,\n         2.0220e-02,  1.2744e-02, -7.5180e-02, -1.6858e-02, -2.3117e-02,\n        -1.8541e-02,  2.1894e-02,  2.4500e-02,  6.4710e-03, -1.6415e-02,\n         2.3707e-03, -4.2507e-02, -2.6368e-02,  9.2107e-03,  1.9542e-02,\n         5.2072e-02, -1.6724e-02, -2.0815e-02, -3.1944e-02,  1.6835e-02,\n        -6.4018e-03, -1.3482e-02, -3.3439e-03, -1.5303e-02, -2.0391e-02,\n         8.5373e-03,  2.8328e-02, -1.6927e-03, -2.2411e-02,  3.0427e-02,\n        -4.7276e-02,  9.2245e-03, -5.0689e-02, -4.8336e-02, -1.2084e-03,\n        -5.2395e-03, -1.6009e-02,  1.6314e-02,  2.2720e-02, -1.4229e-02,\n        -5.8022e-02,  7.6287e-03, -4.1824e-02, -4.0616e-02,  3.3716e-03,\n         1.5608e-02,  8.6987e-03,  3.2286e-02, -9.9302e-03, -1.1604e-02,\n         3.3158e-02,  1.6207e-02, -6.2035e-02, -3.2194e-03, -2.4533e-02,\n        -1.9436e-02, -5.4379e-02,  3.9153e-02, -1.7010e-02,  2.2923e-02,\n         3.4052e-02, -1.7859e-02, -2.1927e-02,  1.1245e-02,  5.2764e-02,\n        -9.7780e-04, -3.7147e-02,  9.8103e-03, -2.2042e-02,  4.3604e-02,\n         5.5854e-02, -1.8232e-02,  2.1558e-02,  3.6381e-02, -3.6335e-02,\n         2.3928e-02,  0.0000e+00, -2.0192e-02,  9.1092e-03, -1.3057e-02,\n        -2.9634e-02, -4.8890e-02,  4.9305e-02,  4.4278e-02,  2.4417e-02,\n         4.4707e-02, -7.7209e-02,  8.4635e-03,  1.2190e-02, -1.4390e-03,\n         2.6488e-02,  5.2211e-02, -5.0965e-03, -5.5532e-03,  3.2438e-02,\n         4.3023e-02,  6.7477e-02, -3.8734e-02,  2.2139e-03, -3.1668e-02,\n         2.8688e-03, -5.5947e-03, -6.6048e-02, -3.7687e-02, -1.9837e-02,\n        -1.3620e-02, -8.2421e-03, -2.3297e-02,  4.1584e-02, -3.3868e-02,\n        -2.6719e-02, -2.5819e-02,  1.1194e-02,  2.7005e-02, -2.0949e-02,\n        -4.7645e-02,  1.9902e-02,  3.0741e-02, -3.4896e-02, -8.9017e-03,\n        -1.0152e-02,  2.2932e-02, -1.8168e-02,  1.7254e-02, -1.2269e-03,\n         2.1092e-02, -1.4344e-02, -3.8443e-02, -1.1079e-02,  1.4506e-02,\n        -1.3459e-02,  4.9582e-03, -1.4220e-02,  1.1627e-02, -3.9287e-02,\n         2.1424e-02, -5.1519e-02, -2.0894e-02,  4.4093e-02,  0.0000e+00,\n         5.1611e-02, -4.1478e-02,  1.7065e-03, -7.1721e-02, -1.8629e-02,\n         5.2949e-03,  1.2864e-02,  5.3641e-02, -2.3084e-02,  5.0274e-02,\n        -1.4178e-02, -1.1807e-02, -1.4160e-03,  3.2032e-02, -4.4739e-02,\n        -2.3329e-02, -2.6235e-02,  9.7088e-03,  6.0098e-03, -5.7100e-02,\n         7.7947e-02,  5.9775e-03, -1.0765e-02,  1.9395e-02, -1.6231e-02,\n         1.6775e-02,  2.1576e-02, -4.4771e-02, -1.3652e-03, -2.1848e-02,\n        -4.2110e-03, -5.1196e-02,  2.2374e-02,  1.7065e-04, -2.8043e-02,\n         1.9113e-02,  1.6687e-02,  1.1475e-02, -1.9164e-02,  4.9628e-02,\n         4.9536e-02, -1.0585e-02, -3.7571e-02, -7.4857e-03, -7.3658e-03,\n         1.2453e-02,  2.1678e-04,  2.4620e-02,  1.1323e-02, -3.8808e-02,\n        -3.7198e-02,  2.5146e-02,  3.2632e-02, -2.3799e-02, -1.9155e-02,\n        -3.6511e-02,  5.4240e-02,  4.0053e-02, -3.6806e-03,  1.8943e-02,\n        -1.4275e-02, -4.0011e-02,  1.7840e-02,  4.5048e-02,  1.7065e-04,\n         5.8161e-02, -3.2742e-02,  1.4898e-03,  3.7032e-02,  7.1305e-03,\n        -2.7512e-02, -2.3338e-03, -6.7846e-03,  3.8881e-03,  2.7826e-02,\n        -2.0838e-02, -2.6876e-02,  3.0625e-02,  1.9265e-02, -4.5209e-02,\n        -3.3199e-02,  2.1309e-03,  3.1977e-02, -6.6924e-03,  4.4596e-02,\n         9.9763e-03,  4.3909e-02, -3.1474e-02,  1.7674e-02, -9.5335e-03,\n        -3.5002e-02, -8.6203e-03,  2.1811e-02, -3.3379e-02,  4.0279e-02,\n        -6.5540e-02, -1.2006e-02, -5.4794e-02, -1.0820e-02, -2.1369e-02,\n        -2.4897e-02, -4.0127e-03,  4.8244e-02, -5.0596e-02,  1.1909e-02,\n        -1.0608e-03,  1.8956e-02, -1.2734e-02, -7.5918e-03,  4.6953e-02,\n         1.5709e-02,  2.9246e-02, -3.7119e-02,  1.7416e-02, -1.4593e-02,\n         1.6415e-02,  2.0428e-02,  2.7046e-02,  2.0294e-04,  2.3532e-02,\n        -5.0366e-02, -5.8622e-03,  2.8458e-03, -4.9443e-03,  1.4270e-02,\n        -4.3738e-02, -2.6484e-02,  1.5128e-02, -3.8079e-02, -2.9186e-02,\n        -8.4773e-03,  1.7361e-02,  1.1973e-02, -9.3168e-04,  8.1868e-03,\n        -1.4026e-02,  3.0150e-02, -8.1222e-03,  2.8993e-02, -1.5179e-02,\n         6.4295e-03,  1.0502e-02,  2.3689e-02,  1.5340e-01, -3.3863e-02,\n         9.9486e-03, -7.5503e-03, -5.8852e-03,  2.1424e-02,  4.0500e-02,\n        -9.7918e-03,  7.7947e-02,  4.1409e-02,  2.6534e-02, -2.4510e-02,\n        -9.0723e-03, -2.5367e-04, -4.7552e-02, -4.7783e-02,  2.0737e-02,\n         1.5783e-02,  5.3364e-02,  9.7457e-03,  1.8878e-02,  1.8126e-03,\n         1.3260e-02, -4.0680e-03, -1.1448e-01, -1.2730e-03,  1.1272e-02,\n        -2.0603e-02, -1.2868e-03, -1.8265e-03,  6.6555e-02,  3.7691e-02,\n        -3.2641e-02,  8.3666e-03,  3.7650e-02,  1.1738e-02,  2.6239e-02,\n         2.0977e-02, -6.0144e-02,  2.5907e-02,  1.4625e-02,  1.1932e-02,\n        -4.4232e-02, -6.4387e-03, -9.7641e-03,  3.0588e-02,  2.6442e-02,\n         3.2613e-02,  1.0945e-02, -1.2227e-02, -3.1243e-02,  1.8675e-02,\n        -2.7212e-03, -3.4546e-03,  1.0617e-02,  5.4886e-03, -1.3043e-02,\n         1.4040e-02, -2.8799e-02, -3.7207e-02, -6.3649e-03, -1.5912e-03,\n        -5.8714e-02, -2.3910e-02, -2.1341e-02,  1.6992e-02, -8.1499e-03,\n        -4.2156e-03, -1.2116e-02, -2.2351e-02,  8.7356e-03,  6.5863e-02,\n        -2.4108e-02, -1.7282e-02,  4.7414e-02,  4.5864e-02, -4.7783e-02,\n        -2.6027e-02,  3.9665e-02, -4.0920e-02, -3.5547e-02, -2.0469e-02,\n        -3.2941e-02,  2.6525e-02, -4.7091e-02, -5.1657e-02, -1.8749e-02,\n         1.6742e-02,  3.6312e-02, -2.0202e-03,  3.9384e-02,  1.3519e-02,\n         4.3125e-03, -2.5829e-03, -3.4454e-03, -2.1839e-02, -6.9045e-03,\n        -5.4609e-02,  2.1747e-02,  3.1271e-03, -5.7745e-02, -2.9030e-02,\n        -3.8969e-02, -1.1254e-02,  6.4941e-02, -3.2152e-02,  1.5313e-03,\n        -2.9565e-03, -4.0131e-02,  8.0715e-02, -3.9573e-02,  4.5897e-02,\n         1.1683e-02,  2.3988e-02,  3.0856e-02, -2.1876e-02, -1.4773e-02,\n         6.1251e-03,  2.5063e-02,  2.1295e-02, -3.7235e-02, -3.8291e-02,\n        -1.3329e-02,  2.2923e-02, -4.1686e-02,  3.7184e-02,  1.9883e-02,\n         2.6917e-02,  4.9259e-03,  6.1066e-02,  4.6768e-02,  3.4675e-02,\n        -2.7337e-02, -2.1977e-02,  1.7241e-02,  4.9351e-02,  3.4633e-02,\n         2.0386e-02, -1.3067e-02,  5.1611e-02, -5.6270e-02, -6.1389e-02,\n        -3.2046e-02,  2.7766e-02, -3.0127e-02,  4.3185e-02,  2.6119e-02,\n         1.9141e-03,  1.0802e-02, -6.0005e-03, -3.6714e-03, -1.8560e-02,\n         1.7268e-02,  4.7183e-02, -1.3150e-02, -4.9905e-03,  5.5485e-02,\n        -3.1742e-02,  3.8545e-02,  2.5944e-02,  3.9656e-02, -2.0017e-02,\n        -5.6316e-02,  3.1497e-02,  1.5216e-02,  3.8236e-03,  2.1604e-02,\n        -6.8400e-03,  1.8634e-02,  9.8795e-03, -2.3061e-01,  5.9498e-04,\n        -1.2020e-02, -3.0344e-02,  0.0000e+00,  4.0011e-02, -1.1641e-02,\n         4.3876e-02,  4.5731e-02,  2.3103e-02, -5.2441e-02,  5.8114e-04,\n        -3.2535e-02, -9.3352e-03, -7.7440e-03,  3.7696e-02,  2.4445e-04,\n         3.8881e-03,  5.2903e-02,  2.5003e-02,  7.7163e-02,  6.4618e-02,\n        -1.9247e-02,  1.4916e-01, -6.6555e-02, -2.9768e-02, -3.8475e-02,\n        -1.4247e-02,  1.3376e-02,  3.6820e-02,  1.8684e-02,  1.7167e-02,\n        -7.4857e-03, -3.5976e-03, -6.5079e-02,  2.5898e-02, -4.1875e-02,\n         2.6756e-02, -3.0063e-02, -3.3176e-02,  2.7793e-02,  1.2264e-02,\n         6.5079e-02,  1.1480e-02,  1.0147e-03, -3.4680e-02, -2.3979e-02,\n        -5.9913e-02, -6.0374e-03,  6.0144e-02, -6.0052e-03, -2.2245e-02,\n         4.6353e-02, -7.0014e-03, -2.9449e-02, -2.9269e-02,  6.0374e-03,\n         1.5285e-01, -7.7947e-04, -4.6768e-02, -4.8798e-02, -3.6358e-02,\n         5.3410e-02,  3.6538e-02,  3.8189e-03, -3.4352e-02, -2.4085e-02,\n        -1.0673e-02, -1.0041e-02, -1.8504e-02,  1.3389e-02, -2.5557e-02,\n        -1.8541e-02,  1.3251e-02, -3.7636e-02, -8.3989e-02,  6.2542e-03,\n        -1.9228e-02,  4.9997e-02,  2.4224e-02, -4.1381e-02, -4.0989e-02,\n        -8.7356e-03, -2.7074e-02, -9.9486e-03, -5.4655e-02,  7.2320e-02,\n        -3.5943e-02, -3.1917e-03,  3.9273e-02,  9.2845e-03,  6.1804e-02,\n        -6.4710e-03,  3.2286e-02, -3.1700e-02,  4.5417e-02,  1.9833e-03,\n        -4.7230e-03,  1.7660e-02,  5.8576e-02,  3.0303e-02,  3.7262e-02,\n        -8.7771e-03, -6.0836e-02,  1.1577e-02,  2.5612e-02,  4.2525e-03,\n        -7.1490e-04,  1.0931e-03,  3.0593e-02, -3.7355e-02, -1.7190e-02,\n         8.6065e-03,  1.0507e-02,  1.0917e-02,  1.7988e-04, -1.0129e-02,\n         1.4699e-02,  6.9184e-05, -6.3511e-02,  1.3749e-02,  3.6833e-02,\n         2.8130e-02, -8.5373e-03, -2.3075e-02,  3.9748e-02, -2.9482e-02,\n         3.5713e-02,  2.1959e-02,  1.0604e-02, -4.1649e-02,  1.8583e-02,\n         4.1192e-02,  1.2315e-02, -1.3874e-02,  5.3318e-02,  5.2165e-02,\n         3.0450e-02,  1.0839e-03,  1.1849e-02, -2.6294e-02,  5.2211e-02,\n         2.3975e-02,  4.3032e-03,  6.2496e-03, -2.5912e-02,  1.1180e-02,\n         9.1922e-03,  3.6026e-02,  1.5640e-02,  2.1401e-03,  1.1171e-02,\n        -2.8287e-02,  5.4332e-02, -6.3741e-03,  2.8923e-02, -1.6613e-02,\n         3.4869e-03,  5.8161e-03,  2.8970e-02,  9.8010e-03,  3.7631e-02,\n        -6.5171e-03,  7.0245e-02, -5.3318e-02, -1.2139e-02,  4.1861e-02,\n         6.7800e-02,  2.0239e-02,  2.5432e-02, -1.3210e-02,  2.8638e-02,\n         9.0769e-03,  1.1434e-02,  3.3208e-03, -2.2167e-02,  2.8596e-04,\n        -2.7489e-02, -3.2549e-02,  1.0087e-02,  3.3467e-02,  4.0726e-03,\n        -1.9800e-02,  2.9657e-02, -1.0253e-02,  1.6290e-02, -2.2969e-03,\n         6.2404e-02])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(label[0]))\nprint(label[0])   #label for 1st feature data","execution_count":21,"outputs":[{"output_type":"stream","text":"tensor(1.)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Verify if the weight are being updated.\n## Unit test for weight updating or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model) #Get Model Parameters","execution_count":22,"outputs":[{"output_type":"stream","text":"MultiLabelClassifer(\n  (fc1): Linear(in_features=876, out_features=4000, bias=True)\n  (fc2): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc3): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc4): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc5): Linear(in_features=4000, out_features=206, bias=True)\n  (drop): Dropout(p=0.4, inplace=False)\n  (out): Linear(in_features=206, out_features=206, bias=True)\n  (sig): Sigmoid()\n  (Log_Sig): LogSigmoid()\n  (soft): Softmax(dim=None)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print model parameters\nfor param in model.parameters():\n    print(param.shape)","execution_count":23,"outputs":[{"output_type":"stream","text":"torch.Size([4000, 876])\ntorch.Size([4000])\ntorch.Size([4000, 4000])\ntorch.Size([4000])\ntorch.Size([4000, 4000])\ntorch.Size([4000])\ntorch.Size([4000, 4000])\ntorch.Size([4000])\ntorch.Size([206, 4000])\ntorch.Size([206])\ntorch.Size([206, 206])\ntorch.Size([206])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Shape\nfor name, param in model.named_parameters():\n    print(name, '\\t\\t', param.shape)\n","execution_count":24,"outputs":[{"output_type":"stream","text":"fc1.weight \t\t torch.Size([4000, 876])\nfc1.bias \t\t torch.Size([4000])\nfc2.weight \t\t torch.Size([4000, 4000])\nfc2.bias \t\t torch.Size([4000])\nfc3.weight \t\t torch.Size([4000, 4000])\nfc3.bias \t\t torch.Size([4000])\nfc4.weight \t\t torch.Size([4000, 4000])\nfc4.bias \t\t torch.Size([4000])\nfc5.weight \t\t torch.Size([206, 4000])\nfc5.bias \t\t torch.Size([206])\nout.weight \t\t torch.Size([206, 206])\nout.bias \t\t torch.Size([206])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.fc1)\n#Weight Before Model train\nprint(\"Weight Of Model Before train is: \")\nprint(model.fc1.weight)","execution_count":25,"outputs":[{"output_type":"stream","text":"Linear(in_features=876, out_features=4000, bias=True)\nWeight Of Model Before train is: \nParameter containing:\ntensor([[-0.0324,  0.0287, -0.0165,  ..., -0.0183,  0.0267, -0.0308],\n        [-0.0147, -0.0139,  0.0003,  ..., -0.0187, -0.0154,  0.0194],\n        [-0.0293, -0.0105, -0.0217,  ...,  0.0160,  0.0012,  0.0163],\n        ...,\n        [ 0.0239, -0.0180,  0.0333,  ..., -0.0230, -0.0163, -0.0230],\n        [-0.0122, -0.0337,  0.0221,  ..., -0.0277, -0.0140, -0.0122],\n        [-0.0004,  0.0154,  0.0274,  ...,  0.0007,  0.0043, -0.0187]],\n       requires_grad=True)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Train model for one batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nbatch = next(iter(train_loader))\nfeature, label = batch\nloss_fn = nn.BCEWithLogitsLoss()\n\ncriterion = torch.nn.BCELoss()\noptimizer = optim.Adam(model.parameters() , .01)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model(feature)\nloss = loss_fn(preds , label)\nloss.backward()\noptimizer.step() #update weights","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Weight After model train\nprint(model.fc1.weight)","execution_count":28,"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[-0.0324,  0.0190, -0.0261,  ..., -0.0228,  0.0319, -0.0240],\n        [-0.0053, -0.0229,  0.0097,  ..., -0.0117, -0.0091,  0.0267],\n        [-0.0293, -0.0009, -0.0311,  ...,  0.0178,  0.0032,  0.0210],\n        ...,\n        [ 0.0139, -0.0279,  0.0233,  ..., -0.0160, -0.0127, -0.0164],\n        [-0.0024, -0.0433,  0.0123,  ..., -0.0349, -0.0179, -0.0192],\n        [ 0.0087,  0.0249,  0.0175,  ..., -0.0080,  0.0095, -0.0137]],\n       requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print prediction\nprd = model(feature)\nprint((label[0])) #lable for one data set\n\nprint(prd[0]) #NOTE: if Sigmoid is not used as final layer activation funciton ==> these prediction will not be range [0,1]. Which is wrong, as output lable in in range [0,1]","execution_count":29,"outputs":[{"output_type":"stream","text":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\ntensor([-1465.7537, -1509.0543, -1532.8311, -1506.2460, -1511.9248, -1465.7925,\n        -1450.2509, -1518.7456, -1437.8958, -1496.1677, -1483.4031, -1494.9773,\n        -1557.2070, -1536.7339, -1576.1472, -1477.8450, -1514.3967, -1525.1572,\n        -1525.0055, -1507.1760, -1515.5663, -1527.8052, -1539.5579, -1453.8853,\n        -1538.5471, -1452.7090, -1478.5243, -1463.2712, -1510.2534, -1491.3291,\n        -1470.8745, -1547.0498, -1507.0164, -1472.2007, -1482.9473, -1532.2832,\n        -1505.0740, -1410.4681, -1627.5391, -1532.6326, -1497.8148, -1501.9209,\n        -1540.8711, -1540.3550, -1568.0308, -1592.9568, -1582.0402, -1471.1960,\n        -1535.2969, -1433.1880, -1571.6667, -1556.8083, -1536.5902, -1562.4277,\n        -1538.4850, -1506.3999, -1473.3601, -1486.9897, -1563.1904, -1453.5664,\n        -1484.0007, -1591.1300, -1473.8071, -1463.0901, -1513.8909, -1521.0339,\n        -1589.0076, -1499.9231, -1558.8376, -1562.5017, -1503.2395, -1480.1063,\n        -1481.9380, -1515.6027, -1578.6211, -1526.6582, -1451.0662, -1460.4005,\n        -1549.1932, -1538.4834, -1448.6738, -1555.7964, -1504.2772, -1534.8555,\n        -1516.4554, -1460.3209, -1551.4948, -1497.0240, -1500.8988, -1488.3113,\n        -1526.0095, -1599.2712, -1531.9487, -1542.7012, -1547.2351, -1382.1045,\n        -1431.7859, -1591.6150, -1498.1572, -1558.9799, -1509.2114, -1501.3428,\n        -1487.2504, -1540.4312, -1548.6396, -1535.1459, -1480.1692, -1547.3107,\n        -1504.2021, -1446.7358, -1512.7588, -1618.8956, -1449.0637, -1592.4572,\n        -1510.5560, -1529.4309, -1486.7532, -1466.2900, -1516.5842, -1498.3218,\n        -1555.2307, -1561.9773, -1484.0238, -1557.7490, -1502.8125, -1564.4984,\n        -1510.3208, -1536.0654, -1499.5449, -1483.3215, -1486.3474, -1520.7332,\n        -1553.4026, -1493.6183, -1453.7500, -1601.4597, -1557.2524, -1565.0295,\n        -1486.1139, -1544.9119, -1472.7683, -1526.2244, -1506.5544, -1551.7996,\n        -1486.8286, -1482.7556, -1579.9695, -1538.6824, -1547.1829, -1512.3374,\n        -1498.4314, -1459.6226, -1439.3757, -1532.0846, -1533.0784, -1597.4343,\n        -1509.8639, -1576.0316, -1548.9192, -1496.0453, -1569.9802, -1512.8792,\n        -1573.4222, -1497.2499, -1478.4180, -1628.7784, -1490.2727, -1511.6179,\n        -1492.4614, -1470.1050, -1580.4911, -1550.3879, -1576.2013, -1538.6646,\n        -1501.2280, -1524.1826, -1596.1704, -1423.7642, -1497.8854, -1555.8247,\n        -1542.5212, -1561.0222, -1619.5699, -1603.8917, -1538.2987, -1506.7994,\n        -1496.9840, -1478.3496, -1547.3169, -1507.9302, -1483.3594, -1560.6941,\n        -1493.1608, -1550.7189, -1534.8990, -1541.5784, -1426.3242, -1440.3306,\n        -1418.5728, -1566.1638, -1486.9067, -1560.6875, -1517.4220, -1513.2412,\n        -1483.6294, -1571.7363], grad_fn=<SelectBackward>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"{device} is used\")\nmodel.to(device)","execution_count":30,"outputs":[{"output_type":"stream","text":"cpu is used\n","name":"stdout"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"MultiLabelClassifer(\n  (fc1): Linear(in_features=876, out_features=4000, bias=True)\n  (fc2): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc3): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc4): Linear(in_features=4000, out_features=4000, bias=True)\n  (fc5): Linear(in_features=4000, out_features=206, bias=True)\n  (drop): Dropout(p=0.4, inplace=False)\n  (out): Linear(in_features=206, out_features=206, bias=True)\n  (sig): Sigmoid()\n  (Log_Sig): LogSigmoid()\n  (soft): Softmax(dim=None)\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.optim as optim\nlearning_rate = [ .01]\n\nfor lr in learning_rate:\n    optimizer = optim.Adam(model.parameters() , lr = lr)\n    #criterion = torch.nn.BCELoss()\n    criterion = nn.BCEWithLogitsLoss()\n    #Difference b/w above two loss => https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586\n    \n    \n          \n    for epoch in range(30):\n        model.train()\n        total_loss = 0\n        total_correct = 0\n\n        for batch in train_loader:\n            features , labels  = batch\n            \n            features = features.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            \n            preds = model(features)\n\n            #print(sum(preds>1))\n            loss = criterion((preds) , labels)\n\n\n            loss.backward()\n            optimizer.step() #update weights\n\n            total_loss += loss.data.item()\n            #print(\"total_loss: {}\".format(total_loss))\n            #total_correct  += get_num_correct(preds,labels)\n        print(\" epoch: {} , learning_rate:{} , total_loss: {}\".format(epoch, lr, total_loss))","execution_count":31,"outputs":[{"output_type":"stream","text":" epoch: 0 , learning_rate:0.01 , total_loss: 10.102628288790584\n epoch: 1 , learning_rate:0.01 , total_loss: 2.9404651364311576\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getPrediction(test):\n    all_pred = np.zeros((test.shape[0] , 206))\n    \n    nfold = 1\n    for i in range(nfold):\n        input_size = train.shape[1]\n        output_size = target.shape[1]\n        \n        hidden_dim = 1024\n        \n        #model = MultiLabelClassifer(input_size , hidden_dim , output_size)\n        #model.load_state_dict() #use if want to load model from memory\n        #model = MultiLabelClassifer(input_size,hidden_dim,output_size)\n        #model.load_state_dict(torch.load(f\"model{i}.bin\"))\n        \n        predictions = list()\n        \n        test_tensor = torch.tensor(test , dtype = torch.float)\n        test_dl  = torch.utils.data.DataLoader(test_tensor , batch_size = 64 )\n        \n        with torch.no_grad(): #stop weight update\n            for i, inputs in enumerate(test_dl):\n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                predictions.extend(outputs.sigmoid().cpu().detach().numpy())\n        \n        all_pred += np.array(predictions) / nfold\n    return all_pred\n\n","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = getPrediction(test)","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\ncreteria = nn.BCEWithLogitsLoss()","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Accurray"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nprint(\"predictions shap is {}, test_target {}\".format(predictions.shape,test_target.shape))\ntest_target_np  =(test_target.iloc[:,1:]).astype(np.float32).to_numpy()\nprint(\"type: {}\".format(type(test_target_np)))\n\n#accurray_test = creteria( torch.from_numpy(predictions.astype(np.float32)) , torch.from_numpy(train_target.iloc[:,1:].astype(np.float32).to_numpy()))\naccurray_test = creteria( torch.from_numpy(predictions.astype(np.float32)) , torch.from_numpy(test_target_np))\nprint(\"accurray on test data is: {}\".format(accurray_test))","execution_count":58,"outputs":[{"output_type":"stream","text":"predictions shap is (4763, 206), test_target (4763, 207)\ntype: <class 'numpy.ndarray'>\naccurray on test data is: 0.6944521069526672\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample.iloc[:,1:] = predictions\n#sample.to_csv('submission.csv',index=False)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}