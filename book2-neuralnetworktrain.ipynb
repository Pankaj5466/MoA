{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/test_features.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Read Input Files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa', 'train_features.csv'))\ntest_data = pd.read_csv(os.path.join('/kaggle/input/lish-moa' , 'test_features.csv'))\n\ntrain_target = pd.read_csv(os.path.join('/kaggle/input/lish-moa','train_targets_scored.csv'))","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Print Shape of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_data.shap: {}  , train_target.shap: {}  , test_data.shap: {} \".format(train_data.shape , train_target.shape , test_data.shape))","execution_count":3,"outputs":[{"output_type":"stream","text":"train_data.shap: (23814, 876)  , train_target.shap: (23814, 207)  , test_data.shap: (3982, 876) \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGTypeFeature = sum(train_data.columns.to_series().str.contains('g-') == True )\nNCTypeFeature = sum(train_data.columns.to_series().str.contains('c-') == True )\n\nprint(\"NGTypeFeature = {} \\nNCTypeFeature = {}\".format(NGTypeFeature , NCTypeFeature))","execution_count":4,"outputs":[{"output_type":"stream","text":"NGTypeFeature = 772 \nNCTypeFeature = 100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Define a Neural Network To Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\nclass MultiLabelClassifer(nn.Module):\n    #TO-DO: Documentation\n    def __init__(self , input_feature , hidden_dim , output_dim):\n        super(MultiLabelClassifer , self).__init__()\n        \n        #nn.utils.weight_norm\n        self.fc1 = (nn.Linear(in_features = input_feature , out_features = 2000))\n        #TO-DO: As close features are related here, convoluation layer will help here\n        #TO-DO: Later add and play with Convolutatin layer, to see accurracy difference\n        self.fc2 =  (nn.Linear( in_features = 2000 , out_features  = 4000))\n        self.fc3 = (nn.Linear( in_features = 4000 , out_features  = 1000))\n        self.fc4 =  (nn.Linear(in_features = 1000 , out_features = 400))\n        self.fc5 = (nn.Linear(in_features = 400 , out_features = output_dim))\n        self.drop = nn.Dropout(0.4) # dropout with 30% prob\n        \n        self.out = (nn.Linear(output_dim,output_dim)) #Above using weight normalization for final layer\n        #Query: Why use weight normalization. Checkout this awesome stackoverlfow post:\n        #link: https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn\n        \n        self.sig = nn.Sigmoid()\n        #self.F_Sig = F.sigmoid() #signout output range from [0,1]\n        self.Log_Sig = nn.LogSigmoid() #log of range 0-1 is whole number?\n        self.soft = nn.Softmax()  #sum of all output is '1'. So its used only in multi-class problem.\n        #self.linear3 = nn.utils.weight_norm(nn.Linear(hidden_size,output_size))\n        #TO-DO: Define output layer so that it be as per Question requirement\n    def forward(self, x):\n        \n        # (1) input layer\n        \n        t = x\n        \n        \n        # (2) Hidden Linear Layer\n        \n        t = self.fc1(t)\n        t = F.relu(t)\n        #3\n        \n        t = self.fc2(t)\n        t = F.relu(t)\n        #4\n        \n        t= self.fc3(t)\n        t = F.relu(t)\n        t = self.drop(t)\n        \n        #5\n        \n        t = self.fc4(t)\n        t = F.relu(t)\n        #6\n        \n        t = self.drop(t)\n        \n        t = self.fc5(t)\n        t = F.relu(t)\n        #t = self.F_Sig(t)\n        #t = F.sigmoid(t)\n        #t = self.Log_Sig(t)\n        \n        # (7) Output Layer\n        \n        t = self.out(t) #This working okay. but why? \n        \n        #t = self.sig(t)  #Using Sigmmoid ALSO as last layer is causing training loss to not decrease\n        #t=self.soft(t) #==> Using softmax as last layer is causing training loss to not decrease\n        \n        return t\n        ","execution_count":112,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rf = train_data.copy()\ntrain_data.head()\n\ntest_rf = test_data.copy()\ntrain_rf.head()","execution_count":113,"outputs":[{"output_type":"execute_result","execution_count":113,"data":{"text/plain":"         sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n0  id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n1  id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n2  id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n3  id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n4  id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n\n      g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n0 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n1  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n2 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n3  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n4  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n\n     c-96    c-97    c-98    c-99  \n0 -0.3981  0.2139  0.3801  0.4176  \n1  0.1522  0.1241  0.6077  0.7371  \n2 -0.6417 -0.2187 -1.4080  0.6931  \n3 -1.6210 -0.8784 -0.3876 -0.8154  \n4  0.1094  0.2885 -0.3786  0.7125  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## One hot coding of input feature and Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data\nimport torch\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,Normalizer\n\nignore_columns = ['sig_id','cp_type']\n\ntrain_columns = [x for x in train_data.columns if x not in ignore_columns]\nprint(len(train_columns))\ntrain = train_rf[train_columns]\nidx = train.index\ncol = train.columns\nprint(\"size of train is {}\",train.shape)\ntrain.head()\n\n\n\n","execution_count":114,"outputs":[{"output_type":"stream","text":"874\nsize of train is {} (23814, 874)\n","name":"stdout"},{"output_type":"execute_result","execution_count":114,"data":{"text/plain":"   cp_time cp_dose     g-0     g-1     g-2     g-3     g-4     g-5     g-6  \\\n0       24      D1  1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120 -1.0220   \n1       72      D1  0.0743  0.4087  0.2991  0.0604  1.0190  0.5207  0.2341   \n2       48      D1  0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390  0.1715   \n3       48      D1 -0.5138 -0.2491 -0.2656  0.5288  4.0620 -0.8095 -1.9590   \n4       72      D2 -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244 -0.2800   \n\n      g-7  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n0 -0.0326  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n1  0.3372  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899  0.1522   \n2  0.2155  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174 -0.6417   \n3  0.1792  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880 -1.6210   \n4 -0.1498  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031  0.1094   \n\n     c-97    c-98    c-99  \n0  0.2139  0.3801  0.4176  \n1  0.1241  0.6077  0.7371  \n2 -0.2187 -1.4080  0.6931  \n3 -0.8784 -0.3876 -0.8154  \n4  0.2885 -0.3786  0.7125  \n\n[5 rows x 874 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>g-6</th>\n      <th>g-7</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>-1.0220</td>\n      <td>-0.0326</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>0.2341</td>\n      <td>0.3372</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>0.1715</td>\n      <td>0.2155</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>-1.9590</td>\n      <td>0.1792</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>-0.2800</td>\n      <td>-0.1498</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 874 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_rf[train_columns]\ntarget = train_target.iloc[:,1:].values\n#https://stackoverflow.com/questions/54160370/how-to-use-sklearn-column-transformer/54160620\ntransform = ColumnTransformer([('o',OneHotEncoder(),[0,1]),('s',Normalizer(),list(range(3,train.shape[1])))])\nprint(\"size before transform train.shape:{} test.shape{}\".format(train.shape,test.shape))\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)\n\nprint(\"size AFTER transform train.shape:{} test.shape{}\".format(train.shape,test.shape))","execution_count":115,"outputs":[{"output_type":"stream","text":"size before transform train.shape:(23814, 874) test.shape(3982, 874)\nsize AFTER transform train.shape:(23814, 876) test.shape(3982, 876)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_rf['cp_dose'] = train_data['cp_dose'].replace({\"D1\":1 , \"D2\":2})\n#train_rf['cp_type'] = train_rf['cp_type'].replace({\"trt_cp\":1 , \"ctl_vehicle\":2})\n\n#test_rf['cp_dose'] = test_data['cp_dose'].replace({\"D1\":1 , \"D2\":2})\n#test_rf['cp_type'] = test_data['cp_type'].replace({\"trt_cp\":1 , \"ctl_vehicle\":2})\nprint(type(train))\nprint(len(col))\nprint(len(idx))\nprint(train.shape)\nprint(type(col))\n\n#col = pd.concat(['p1', 'p2'] ,col)\ntrain_df = pd.DataFrame(data = train,index = idx ) #Check if encoding is successful or not\n","execution_count":116,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n874\n23814\n(23814, 876)\n<class 'pandas.core.indexes.base.Index'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":117,"outputs":[{"output_type":"execute_result","execution_count":117,"data":{"text/plain":"   0    1    2    3    4         5         6         7         8         9    \\\n0  1.0  0.0  0.0  1.0  0.0  0.025030 -0.011126 -0.027862 -0.008725 -0.045420   \n1  0.0  0.0  1.0  1.0  0.0  0.018548  0.013574  0.002741  0.046245  0.023631   \n2  0.0  1.0  0.0  1.0  0.0  0.020432  0.054585 -0.002684 -0.001135  0.043520   \n3  0.0  1.0  0.0  1.0  0.0 -0.006312 -0.006730  0.013400  0.102930 -0.020513   \n4  0.0  0.0  1.0  0.0  1.0 -0.014177  0.034302  0.024468  0.050145 -0.029153   \n\n   ...       866       867       868       869       870       871       872  \\\n0  ...  0.012845  0.011597  0.036246  0.024788 -0.008581  0.029550 -0.017867   \n1  ... -0.019356  0.034232  0.021366  0.001044  0.013420  0.022233  0.006907   \n2  ... -0.025466 -0.022118  0.021437  0.000783 -0.046506 -0.011149 -0.022540   \n3  ... -0.053188 -0.016321 -0.142663 -0.034918 -0.021873 -0.032638 -0.041076   \n4  ...  0.000149  0.000170  0.023587  0.037803  0.019531 -0.010719  0.003869   \n\n        873       874       875  \n0  0.009600  0.017059  0.018742  \n1  0.005632  0.027579  0.033451  \n2 -0.007682 -0.049456  0.024345  \n3 -0.022259 -0.009822 -0.020662  \n4  0.010202 -0.013388  0.025196  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>866</th>\n      <th>867</th>\n      <th>868</th>\n      <th>869</th>\n      <th>870</th>\n      <th>871</th>\n      <th>872</th>\n      <th>873</th>\n      <th>874</th>\n      <th>875</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.025030</td>\n      <td>-0.011126</td>\n      <td>-0.027862</td>\n      <td>-0.008725</td>\n      <td>-0.045420</td>\n      <td>...</td>\n      <td>0.012845</td>\n      <td>0.011597</td>\n      <td>0.036246</td>\n      <td>0.024788</td>\n      <td>-0.008581</td>\n      <td>0.029550</td>\n      <td>-0.017867</td>\n      <td>0.009600</td>\n      <td>0.017059</td>\n      <td>0.018742</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.018548</td>\n      <td>0.013574</td>\n      <td>0.002741</td>\n      <td>0.046245</td>\n      <td>0.023631</td>\n      <td>...</td>\n      <td>-0.019356</td>\n      <td>0.034232</td>\n      <td>0.021366</td>\n      <td>0.001044</td>\n      <td>0.013420</td>\n      <td>0.022233</td>\n      <td>0.006907</td>\n      <td>0.005632</td>\n      <td>0.027579</td>\n      <td>0.033451</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.020432</td>\n      <td>0.054585</td>\n      <td>-0.002684</td>\n      <td>-0.001135</td>\n      <td>0.043520</td>\n      <td>...</td>\n      <td>-0.025466</td>\n      <td>-0.022118</td>\n      <td>0.021437</td>\n      <td>0.000783</td>\n      <td>-0.046506</td>\n      <td>-0.011149</td>\n      <td>-0.022540</td>\n      <td>-0.007682</td>\n      <td>-0.049456</td>\n      <td>0.024345</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.006312</td>\n      <td>-0.006730</td>\n      <td>0.013400</td>\n      <td>0.102930</td>\n      <td>-0.020513</td>\n      <td>...</td>\n      <td>-0.053188</td>\n      <td>-0.016321</td>\n      <td>-0.142663</td>\n      <td>-0.034918</td>\n      <td>-0.021873</td>\n      <td>-0.032638</td>\n      <td>-0.041076</td>\n      <td>-0.022259</td>\n      <td>-0.009822</td>\n      <td>-0.020662</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.014177</td>\n      <td>0.034302</td>\n      <td>0.024468</td>\n      <td>0.050145</td>\n      <td>-0.029153</td>\n      <td>...</td>\n      <td>0.000149</td>\n      <td>0.000170</td>\n      <td>0.023587</td>\n      <td>0.037803</td>\n      <td>0.019531</td>\n      <td>-0.010719</td>\n      <td>0.003869</td>\n      <td>0.010202</td>\n      <td>-0.013388</td>\n      <td>0.025196</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Get Data Tensor"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train))\ntrain_data_tensor =  torch.from_numpy(train.astype(np.float32))\ntrain_target_tensor = torch.from_numpy(target.astype(np.float32))","execution_count":118,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"max_val {}, min_val {}\",train_data_tensor.max() , train_data_tensor.min() )\n#Value range b/w -1 to 1 implies that it has been normalized","execution_count":119,"outputs":[{"output_type":"stream","text":"max_val {}, min_val {} tensor(1.) tensor(-0.4424)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_target_tensor)","execution_count":120,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sum(train_target_tensor)))","execution_count":121,"outputs":[{"output_type":"stream","text":"tensor(16844.)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"type of train_data_tensor: {}\",type(train_data_tensor) )\ntrain_set = torch.utils.data.TensorDataset(train_data_tensor , train_target_tensor)\nprint(train_set)","execution_count":122,"outputs":[{"output_type":"stream","text":"type of train_data_tensor: {} <class 'torch.Tensor'>\n<torch.utils.data.dataset.TensorDataset object at 0x7f7fb713d290>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Get dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 120\ntrain_loader = torch.utils.data.DataLoader(train_set , batch_size = batch_size, shuffle=True)\nprint(train_loader)","execution_count":123,"outputs":[{"output_type":"stream","text":"<torch.utils.data.dataloader.DataLoader object at 0x7f7fb713d350>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Now take Instance of Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultiLabelClassifer(876 , 1024  , 206 )\n#(self , input_feature , hidden_dim , output_dim):\n#model = ModelTWO(876 , 206 , 1024)","execution_count":124,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyser Sample Formed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(train_loader))\nfeature, label = batch","execution_count":125,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(batch[0].shape)\nprint(batch[0][0].shape) #THis is one extracted Feature out of 100","execution_count":126,"outputs":[{"output_type":"stream","text":"torch.Size([120, 876])\ntorch.Size([876])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(feature)","execution_count":127,"outputs":[{"output_type":"stream","text":"tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.1060e-02,\n          7.5089e-02, -1.1622e-02],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.3246e-02,\n          3.0805e-04,  7.0302e-03],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  5.9820e-02,\n         -4.4562e-03,  1.3422e-02],\n        ...,\n        [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.1282e-02,\n         -2.9852e-02, -1.1367e-02],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ..., -5.6313e-02,\n         -3.1234e-02, -5.0670e-02],\n        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ..., -1.6008e-02,\n         -4.9218e-02,  6.4635e-04]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(label[0]))\nprint(label[0])   #label for 1st feature data","execution_count":128,"outputs":[{"output_type":"stream","text":"tensor(1.)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Verify if the weight are being updated.\n## Unit test for weight updating or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":129,"outputs":[{"output_type":"stream","text":"MultiLabelClassifer(\n  (fc1): Linear(in_features=876, out_features=2000, bias=True)\n  (fc2): Linear(in_features=2000, out_features=4000, bias=True)\n  (fc3): Linear(in_features=4000, out_features=1000, bias=True)\n  (fc4): Linear(in_features=1000, out_features=400, bias=True)\n  (fc5): Linear(in_features=400, out_features=206, bias=True)\n  (drop): Dropout(p=0.4, inplace=False)\n  (out): Linear(in_features=206, out_features=206, bias=True)\n  (sig): Sigmoid()\n  (Log_Sig): LogSigmoid()\n  (soft): Softmax(dim=None)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print model parameters\nfor param in model.parameters():\n    print(param.shape)","execution_count":130,"outputs":[{"output_type":"stream","text":"torch.Size([2000, 876])\ntorch.Size([2000])\ntorch.Size([4000, 2000])\ntorch.Size([4000])\ntorch.Size([1000, 4000])\ntorch.Size([1000])\ntorch.Size([400, 1000])\ntorch.Size([400])\ntorch.Size([206, 400])\ntorch.Size([206])\ntorch.Size([206, 206])\ntorch.Size([206])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Shape\nfor name, param in model.named_parameters():\n    print(name, '\\t\\t', param.shape)\n","execution_count":131,"outputs":[{"output_type":"stream","text":"fc1.weight \t\t torch.Size([2000, 876])\nfc1.bias \t\t torch.Size([2000])\nfc2.weight \t\t torch.Size([4000, 2000])\nfc2.bias \t\t torch.Size([4000])\nfc3.weight \t\t torch.Size([1000, 4000])\nfc3.bias \t\t torch.Size([1000])\nfc4.weight \t\t torch.Size([400, 1000])\nfc4.bias \t\t torch.Size([400])\nfc5.weight \t\t torch.Size([206, 400])\nfc5.bias \t\t torch.Size([206])\nout.weight \t\t torch.Size([206, 206])\nout.bias \t\t torch.Size([206])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.fc1)\n#Weight Before Model train\nprint(model.fc1.weight)","execution_count":132,"outputs":[{"output_type":"stream","text":"Linear(in_features=876, out_features=2000, bias=True)\nParameter containing:\ntensor([[ 0.0081,  0.0327, -0.0197,  ..., -0.0287, -0.0120, -0.0078],\n        [-0.0303,  0.0296, -0.0299,  ...,  0.0178,  0.0132,  0.0007],\n        [-0.0118, -0.0266, -0.0329,  ..., -0.0321,  0.0110, -0.0142],\n        ...,\n        [-0.0262,  0.0044,  0.0135,  ...,  0.0184, -0.0328,  0.0237],\n        [ 0.0212, -0.0095, -0.0241,  ...,  0.0204,  0.0080,  0.0076],\n        [-0.0013,  0.0298, -0.0049,  ...,  0.0338, -0.0274, -0.0183]],\n       requires_grad=True)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Train model for one batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\nbatch = next(iter(train_loader))\nfeature, label = batch\nloss_fn = nn.BCEWithLogitsLoss()\n\ncriterion = torch.nn.BCELoss()\noptimizer = optim.Adam(model.parameters() , .01)","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model(feature)\nloss = loss_fn(preds , label)\nloss.backward()\noptimizer.step() #update weights","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Weight After model train\nprint(model.fc1.weight)","execution_count":135,"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[-0.0017,  0.0424, -0.0292,  ..., -0.0198, -0.0154, -0.0136],\n        [-0.0206,  0.0395, -0.0299,  ...,  0.0253,  0.0066,  0.0075],\n        [-0.0018, -0.0167, -0.0239,  ..., -0.0292,  0.0185, -0.0151],\n        ...,\n        [-0.0362, -0.0055,  0.0035,  ...,  0.0258, -0.0406,  0.0321],\n        [ 0.0112, -0.0194, -0.0341,  ...,  0.0118,  0.0015,  0.0164],\n        [-0.0112,  0.0397,  0.0050,  ...,  0.0258, -0.0356, -0.0187]],\n       requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print prediction\nprd = model(feature)\nprint((label[0])) #lable for one data set\n\nprint(prd[0]) #NOTE: if Sigmoid is not used as final layer activation funciton ==> these prediction will not be range [0,1]. Which is wrong, as output lable in in range [0,1]","execution_count":136,"outputs":[{"output_type":"stream","text":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\ntensor([-15.9391, -25.4950, -22.7615, -26.5382, -22.0151, -42.7090, -29.3274,\n         -1.8235, -22.7899, -39.1687, -19.9006, -15.5125, -24.9506, -21.1439,\n         -9.7183, -16.7595, -35.1252, -32.6217, -27.8414,  -6.9434, -12.3545,\n        -26.2683, -24.3878, -19.8820, -12.7591, -35.8788, -21.7339, -32.5589,\n         -7.4176, -11.3294, -29.9101, -32.0236, -18.3614, -17.0309, -40.8070,\n        -21.8958, -24.4563, -15.1623, -20.8287, -18.7753, -20.2889, -12.6747,\n        -19.1620,  -6.6907, -29.7928, -16.3981, -33.0112, -27.8927, -29.7547,\n        -13.8205, -30.8249, -21.0335, -23.0239,  -6.3781, -16.6587, -11.8426,\n        -25.7627, -26.6886,  -6.2444, -12.5858, -16.1759, -20.3523, -17.3555,\n        -11.0122, -22.0239, -32.3303, -26.8568, -14.1722, -25.3281, -37.6297,\n        -29.7691, -27.1968, -17.0971, -41.7640, -18.3955, -24.9951,  -1.8933,\n         -8.2171, -18.4691, -33.0754, -24.3382, -31.8449, -14.2961, -20.8946,\n        -25.2766, -30.9565, -20.2894, -28.4257, -25.7158, -21.6654, -32.7209,\n         -5.7454,   2.4630, -32.8275, -18.0354, -14.0299, -29.4856, -13.7743,\n        -14.9210, -11.7852,  -2.9852,  -8.8484, -32.2349, -21.6933, -18.6216,\n        -12.8094, -14.9413,  -5.6300, -38.0415,   2.8534, -27.0255, -21.5706,\n        -28.1918, -41.1912, -19.4861, -16.5954, -18.8634, -30.3452, -17.9830,\n        -18.9373, -33.1745, -15.6735, -26.0328, -17.3779, -28.6687, -16.1093,\n        -24.7120, -20.0424, -38.0584, -38.9410, -35.6255, -18.4060, -21.7250,\n        -26.7180, -16.5628, -20.0662, -18.8496, -27.8232, -26.5080, -31.8754,\n        -33.0266, -28.3275, -24.5188, -15.2918, -13.6713, -28.6886, -30.4123,\n        -18.1500, -28.8765,  -9.5485, -20.8915, -25.9419,  -6.8299, -20.0881,\n        -27.5167,  -3.3265, -14.7489, -12.6394,  -0.5942, -27.5569,  -9.0114,\n        -29.4360, -20.6323, -51.6105, -15.1391, -16.6593, -29.3404, -22.2614,\n        -24.9311, -23.2636, -29.5332, -19.5774, -26.8350, -15.4337, -37.8985,\n        -19.4907, -19.9679, -25.7219, -13.7556,  -8.0786,   3.1991, -27.0160,\n         -7.1213,  -5.5178, -28.3616, -29.7357, -33.9120, -23.6609, -45.7439,\n        -26.2366, -18.3932,  -5.7416, -24.1465, -14.3970, -31.0480, -19.8258,\n        -29.7264, -25.3291, -29.1441, -34.3400, -21.9422, -18.4710, -19.7647,\n        -13.4332, -22.9999,  -8.3994], grad_fn=<SelectBackward>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Now train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.optim as optim\nlearning_rate = [ .01]\n\nfor lr in learning_rate:\n    optimizer = optim.Adam(model.parameters() , lr = lr)\n    #criterion = torch.nn.BCELoss()\n    criterion = nn.BCEWithLogitsLoss()\n    #Difference b/w above two loss => https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586\n\n    for epoch in range(6):\n        model.train()\n        total_loss = 0\n        total_correct = 0\n\n        for batch in train_loader:\n            features , labels  = batch\n            optimizer.zero_grad()\n\n            preds = model(features)\n\n            #print(sum(preds>1))\n            loss = criterion((preds) , labels)\n\n\n            loss.backward()\n            optimizer.step() #update weights\n\n            total_loss += loss.data.item()\n            #print(\"total_loss: {}\".format(total_loss))\n            #total_correct  += get_num_correct(preds,labels)\n        print(\" epoch: {} , learning_rate:{} , total_loss: {}\".format(epoch, lr, total_loss))","execution_count":139,"outputs":[{"output_type":"stream","text":" epoch: 0 , learning_rate:0.01 , total_loss: 5.344890164211392\n epoch: 1 , learning_rate:0.01 , total_loss: 3.781572419218719\n epoch: 2 , learning_rate:0.01 , total_loss: 3.5999912759289145\n epoch: 3 , learning_rate:0.01 , total_loss: 3.5812499364838004\n epoch: 4 , learning_rate:0.01 , total_loss: 3.551726679317653\n epoch: 5 , learning_rate:0.01 , total_loss: 3.580019478686154\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"DONe\")","execution_count":140,"outputs":[{"output_type":"stream","text":"DONe\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"20 epochssddsfdsd\")\n\n","execution_count":142,"outputs":[{"output_type":"stream","text":"20 epochssdfdsd\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}